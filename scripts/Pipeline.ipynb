{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from gatenlp import Document\n",
    "import docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all txt files from folder \n",
    "def read_txt_files(folder_path):\n",
    "    import os\n",
    "    txt_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "    documents = []\n",
    "    for file in txt_files:\n",
    "        with open(os.path.join(folder_path, file), 'r', encoding='utf-8') as f:\n",
    "            filename = os.path.splitext(file)[0]\n",
    "            \n",
    "            content = f.read()\n",
    "            gatenlp_doc = Document(content)\n",
    "            gatenlp_doc.name = filename\n",
    "            documents.append(gatenlp_doc)\n",
    "            \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docx(file_path):\n",
    "    full_text = docx2txt.process(file_path)\n",
    "\n",
    "    return full_text\n",
    "\n",
    "# read all docs from folder and extract text\n",
    "def read_docs_from_folder(folder_path):\n",
    "    import os\n",
    "    files = os.listdir(folder_path)\n",
    "    docs = []\n",
    "    for file in files:\n",
    "        if file.endswith('.docx'):\n",
    "            doc = read_docx(folder_path + \"/\" + file)\n",
    "            gateNlpDoc = Document(doc)\n",
    "            gateNlpDoc.name = file\n",
    "            docs.append(gateNlpDoc)\n",
    "    return docs\n",
    "\n",
    "all_docs = read_docs_from_folder(\"./DocumentsStore/batini3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = read_txt_files(\"./DocumentsStore/bologna_final\")\n",
    "len(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all json files in folder\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import math \n",
    "def read_json_files(folder):\n",
    "    data = []\n",
    "    for filename in glob.glob(os.path.join(folder, '*.json')):\n",
    "        with open (filename, 'r') as f:\n",
    "            jsonDoc = json.load(f)\n",
    "            # for key, value in jsonDoc['features'].items():\n",
    "            #     if isinstance(value, float) and math.isnan(value):\n",
    "            #         jsonDoc['features'][key] = \"NaN\"\n",
    "        \n",
    "            data.append(Document.from_dict(jsonDoc))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerDocs = read_json_files(\n",
    "    \"./DocumentsStore/missing\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nerDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerDocs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_docs[5].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "now = time.time()\n",
    "nerDocs = []\n",
    "for document in tqdm(all_docs):\n",
    "    try:\n",
    "        res = requests.post(\n",
    "            \"http://vm.chronos.disco.unimib.it:10881/api/spacyner\",\n",
    "            json=document.to_dict(),\n",
    "        )\n",
    "        assert res.ok  \n",
    "        nerDocs.append(Document.from_dict(res.json()))\n",
    "    except Exception as e:\n",
    "        print(e)      \n",
    "elapsed = time.time() - now\n",
    "print(elapsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_docs), len(nerDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump processed docs with joblib \n",
    "import joblib\n",
    "joblib.dump(nerDocs, 'nerDocsBologna.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerDocs[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi Encoder Linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsons_from_folder(folder_path, clean_linking_keys=True):\n",
    "    import os\n",
    "\n",
    "    json_files = [f for f in os.listdir(folder_path) if f.endswith(\".json\")]\n",
    "    documents = []\n",
    "    \n",
    "    # Keys to remove from annotations\n",
    "    linking_keys = [\n",
    "        \"linking_probability\", \"linking_score\", \"candidates\", \"linking\", \n",
    "        \"nil_score\", \"nil_probability\", \"nil_label\", \"nil_prediction\"\n",
    "    ]\n",
    "    \n",
    "    for file in json_files:\n",
    "        with open(os.path.join(folder_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            content = json.load(f)\n",
    "            \n",
    "            # Clean the annotations by removing linking keys if requested\n",
    "            if clean_linking_keys and \"annotation_sets\" in content:\n",
    "                for ann_set_name, ann_set in content[\"annotation_sets\"].items():\n",
    "                    if \"annotations\" in ann_set:\n",
    "                        for annotation in ann_set[\"annotations\"]:\n",
    "                            # Remove linking keys from the annotation\n",
    "                            for key in linking_keys:\n",
    "                                if key in annotation.get(\"features\", {}):\n",
    "                                    del annotation[\"features\"][key]\n",
    "            \n",
    "            # Convert to GateNLP document\n",
    "            documents.append(Document.from_dict(content))\n",
    "            \n",
    "    print(f\"Loaded {len(documents)} documents from {folder_path}\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "nerDocs = read_jsons_from_folder(\n",
    "    \"./DocumentsStore/output\",\n",
    "    clean_linking_keys=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load ner docs from joblib\n",
    "import joblib\n",
    "nerDocs = joblib.load('nerDocsBatini.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "now = time.time()\n",
    "linkedDocs = []\n",
    "for document in tqdm(nerDocs):\n",
    "    try: \n",
    "        res = requests.post(\n",
    "            \"http://vm.chronos.disco.unimib.it:10881/api/blink/biencoder/mention/doc\",\n",
    "            json=document.to_dict(),\n",
    "        )\n",
    "        assert res.ok  \n",
    "        linkedDocs.append(Document.from_dict(res.json()))\n",
    "    except Exception as e:\n",
    "        print('error',e)      \n",
    "elapsed = time.time() - now\n",
    "print(elapsed)\n",
    "assert res.ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "now = time.time()\n",
    "indexed = []\n",
    "for document in tqdm(linkedDocs):\n",
    "    try: \n",
    "        res = requests.post(\n",
    "            \"http://vm.chronos.disco.unimib.it:10881/api/indexer/search/doc\",\n",
    "            json=document.to_dict(),\n",
    "        )\n",
    "        indexed.append(Document.from_dict(res.json()))\n",
    "    except Exception as e:\n",
    "        print('error', e)      \n",
    "elapsed = time.time() - now\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nil prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "now = time.time()\n",
    "nilDocs = []\n",
    "for document in tqdm(indexed):\n",
    "    try:\n",
    "        res = requests.post(\n",
    "            \"http://vm.chronos.disco.unimib.it:10881/api/nilprediction/doc\",\n",
    "            json=document.to_dict(),\n",
    "        )\n",
    "\n",
    "        nilDocs.append(Document.from_dict(res.json()))\n",
    "    except Exception as e:\n",
    "        print('error', e)      \n",
    "elapsed = time.time() - now\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nilDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib \n",
    "joblib.dump(nilDocs, 'nilDocsBatini.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read jsons from folder\n",
    "def read_jsons_from_folder(folder_path, clean_linking_keys=True):\n",
    "    import os\n",
    "    json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "    documents = []\n",
    "    \n",
    "    # Keys to remove from annotations\n",
    "    linking_keys = [\n",
    "        \"linking_probability\", \"linking_score\", \"candidates\", \"linking\", \n",
    "        \"nil_score\", \"nil_probability\", \"nil_label\", \"nil_prediction\"\n",
    "    ]\n",
    "    \n",
    "    for file in json_files:\n",
    "        with open(os.path.join(folder_path, file), 'r', encoding='utf-8') as f:\n",
    "            content = json.load(f)\n",
    "            \n",
    "            # Clean the annotations by removing linking keys if requested\n",
    "            if clean_linking_keys and \"annotation_sets\" in content:\n",
    "                for ann_set_name, ann_set in content[\"annotation_sets\"].items():\n",
    "                    if \"annotations\" in ann_set:\n",
    "                        for annotation in ann_set[\"annotations\"]:\n",
    "                            # Remove linking keys from the annotation\n",
    "                            for key in linking_keys:\n",
    "                                if key in annotation.get(\"features\", {}):\n",
    "                                    del annotation[\"features\"][key]\n",
    "            \n",
    "            # Convert to GateNLP document\n",
    "            documents.append(Document.from_dict(content))\n",
    "            \n",
    "    print(f\"Loaded {len(documents)} documents from {folder_path}\")\n",
    "    return documents\n",
    "\n",
    "nilDocs = read_jsons_from_folder(\n",
    "    \"./DocumentsStore/output\",\n",
    "    clean_linking_keys=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "now = time.time()\n",
    "clustering = []\n",
    "\n",
    "for document in tqdm(nilDocs):\n",
    "    try:\n",
    "        res = requests.post(\n",
    "            \"http://vm.chronos.disco.unimib.it:10881/api/clustering\",\n",
    "            json=document.to_dict(),\n",
    "        )\n",
    "        clustering.append(Document.from_dict(res.json()))\n",
    "    except Exception as e:\n",
    "        print('error', e)      \n",
    "elapsed = time.time() - now\n",
    "print(elapsed)\n",
    "assert res.ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(clustering, 'clusteringBatini.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib \n",
    "clustering = joblib.load('clusteringBatini.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "now = time.time()\n",
    "nilConsDocs = []\n",
    "for document in tqdm(clustering):\n",
    "    try:\n",
    "        res = requests.post(\n",
    "            \"http://vm.chronos.disco.unimib.it:10881/api/consolidation\",\n",
    "            json=document.to_dict(),\n",
    "        )\n",
    "\n",
    "        nilConsDocs.append(Document.from_dict(res.json()))\n",
    "    except Exception as e:\n",
    "        print('error', e)      \n",
    "elapsed = time.time() - now\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nilConsDocs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping between documents and their original filenames\n",
    "import os\n",
    "\n",
    "# Get the original filenames from the source directory\n",
    "source_folder = \"./DocumentsStore/bologna_final\"\n",
    "original_filenames = [f for f in os.listdir(source_folder) if f.endswith('.txt')]\n",
    "\n",
    "# Create a mapping dictionary\n",
    "file_to_document_mapping = {}\n",
    "\n",
    "# Match documents with their original filenames\n",
    "for i, document in enumerate(nilConsDocs):\n",
    "    if i < len(original_filenames):\n",
    "        # Get the original filename without extension\n",
    "        original_filename = original_filenames[i]\n",
    "        doc_name = original_filename.replace('.txt', '')\n",
    "        \n",
    "        # Set the document name to the original filename (without extension)\n",
    "        document.name = doc_name\n",
    "        \n",
    "        # Add to mapping\n",
    "        file_to_document_mapping[doc_name] = document\n",
    "        \n",
    "        print(f\"Document {i}: {doc_name}\")\n",
    "    else:\n",
    "        print(f\"Warning: Document {i} has no corresponding filename\")\n",
    "\n",
    "print(f\"\\nCreated mapping for {len(file_to_document_mapping)} documents\")\n",
    "print(\"Sample document names:\", list(file_to_document_mapping.keys())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all documents as json\n",
    "\n",
    "for document in nilDocs:\n",
    "    with open(f\"DocumentsStore/bologna_final/{document.name}.json.annotated\", \"w\") as f:\n",
    "        json.dump(document.to_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(nilConsDocs, './DocumentsStore/finalDocsBatini.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
