{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents from out_anon\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "# Load JSON-annotated documents from DocumentsStore\n",
    "documents_path = Path(\"./out_anon\")\n",
    "json_files: List[Path] = [p for p in documents_path.iterdir() if p.suffixes == ['.json'] or p.name.endswith('.json')]\n",
    "\n",
    "data = []\n",
    "for json_file in json_files:\n",
    "    with json_file.open('r', encoding='utf-8') as f:\n",
    "        fileObject = json.load(f)\n",
    "        data.append(fileObject)\n",
    "\n",
    "print(f'Loaded {len(data)} documents from {documents_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of loaded documents\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 48 text files to DocumentsStore/bolognaSource\n"
     ]
    }
   ],
   "source": [
    "# Save plain text previews for each document\n",
    "from pathlib import Path\n",
    "out_dir = Path('DocumentsStore/bolognaSource')\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "for i, fileObject in enumerate(data):\n",
    "    text = fileObject.get('text', '')\n",
    "    path = out_dir / f'{i}.txt'\n",
    "    path.write_text(text, encoding='utf-8')\n",
    "    # avoid noisy per-file prints in bulk operations\n",
    "print(f'Wrote {len(data)} text files to {out_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: load a single indexed.json file if present\n",
    "from pathlib import Path\n",
    "indexed_file = Path('./DocumentsStore/indexed.json')\n",
    "data = None\n",
    "if indexed_file.exists():\n",
    "    with indexed_file.open('r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        print('Loaded indexed.json')\n",
    "else:\n",
    "    print('indexed.json not found; using previously loaded data if any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally load pre-serialized data (joblib) if you prefer\n",
    "import joblib\n",
    "joblib_path = Path('./DocumentsStore/finalDocsBatini.joblib')\n",
    "if joblib_path.exists():\n",
    "    data = joblib.load(str(joblib_path))\n",
    "    print('Loaded joblib data')\n",
    "else:\n",
    "    print('joblib file not found; skipping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from typing import Dict, Any\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "if not logger.handlers:\n",
    "    # basic config for notebook runs\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def get_string_hash(input_string: str) -> str:\n",
    "    \"\"\"Return a SHA-256 hex digest for a given string.\"\"\"\n",
    "    return hashlib.sha256(input_string.encode('utf-8')).hexdigest()\n",
    "\n",
    "def clean_doc(document: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Remove MongoDB-specific fields from a document in-place and return it.\"\"\"\n",
    "    fields_to_remove = ['_id', 'inc_id', '__v', 'edited']\n",
    "    for field in fields_to_remove:\n",
    "        document.pop(field, None)\n",
    "\n",
    "    # Clean annotation sets safely\n",
    "    ann_sets = document.get('annotation_sets', {})\n",
    "    for annset_name, annset in ann_sets.items():\n",
    "        if isinstance(annset, dict):\n",
    "            annset.pop('_id', None)\n",
    "            annotations = annset.get('annotations', [])\n",
    "            for annotation in annotations:\n",
    "                annotation.pop('_id', None)\n",
    "                annotation.pop('annotationSetId', None)\n",
    "    return document\n",
    "\n",
    "def remove_surrogates(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    encoded_text = text.encode('utf-16', 'surrogatepass')\n",
    "    return encoded_text.decode('utf-16', errors='ignore')\n",
    "\n",
    "def process_doc_for_mongo(obj: Dict[str, Any], database) -> None:\n",
    "    \"\"\"Prepare and insert document, annotation sets and annotations into MongoDB.\"\"\"\n",
    "    try:\n",
    "        text = obj.get('text', '')\n",
    "        doc_id = get_string_hash(text)\n",
    "        document = {\n",
    "            'text': remove_surrogates(text),\n",
    "            'preview': remove_surrogates(text[:100] + '...'),\n",
    "            'name': remove_surrogates(obj.get('name', '')),\n",
    "            'features': obj.get('features', {}),\n",
    "            'offset_type': obj.get('offset_type'),\n",
    "            'id': doc_id,\n",
    "        }\n",
    "        document.pop('_id', None)\n",
    "        database['documents'].insert_one(document)\n",
    "\n",
    "        annotation_sets = obj.get('annotation_sets', {})\n",
    "        annset_collection = database['annotationSets']\n",
    "        annset_id_map = {}\n",
    "        for name, annset in annotation_sets.items():\n",
    "            ann_record = {\n",
    "                'name': name,\n",
    "                'docId': doc_id,\n",
    "                'next_annid': annset.get('next_annid', 1),\n",
    "            }\n",
    "            ann_record.pop('_id', None)\n",
    "            inserted = annset_collection.insert_one(ann_record)\n",
    "            annset_id_map[name] = inserted.inserted_id\n",
    "\n",
    "        annotation_collection = database['annotations']\n",
    "        for name, annset in annotation_sets.items():\n",
    "            for annotation in annset.get('annotations', []):\n",
    "                try:\n",
    "                    annotation['annotationSetId'] = annset_id_map[name]\n",
    "                    if 'features' in annotation and 'mention' in annotation['features']:\n",
    "                        annotation['features']['mention'] = remove_surrogates(annotation['features']['mention'])\n",
    "                    annotation.pop('_id', None)\n",
    "                    annotation_collection.insert_one(annotation)\n",
    "                except Exception as e:\n",
    "                    logger.exception('Error inserting annotation for doc %s: %s', doc_id, e)\n",
    "    except Exception as e:\n",
    "        logger.exception('Error processing document to Mongo: %s', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTITY_PREPROCESSING=False, CLUSTER_MERGING=False, DRY_RUN=True\n"
     ]
    }
   ],
   "source": [
    "# Configuration for entity preprocessing\n",
    "ENABLE_ENTITY_PREPROCESSING = False  # Set to True to enable automatic entity detection\n",
    "MIN_MENTION_LENGTH = 2  # Minimum length for entity mentions to be considered\n",
    "CASE_SENSITIVE = False  # Set to True for case-sensitive matching\n",
    "# Configuration for cluster merging\n",
    "ENABLE_CLUSTER_MERGING = False  # Set to True to enable merging of duplicate annotation sets\n",
    "# Use environment variable to toggle dry-run (won't write to Mongo)\n",
    "DRY_RUN = os.environ.get('DRY_RUN', '1') != '0'\n",
    "print(f'ENTITY_PREPROCESSING={ENABLE_ENTITY_PREPROCESSING}, CLUSTER_MERGING={ENABLE_CLUSTER_MERGING}, DRY_RUN={DRY_RUN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for saving enhanced documents\n",
    "SAVE_ENHANCED_DOCUMENTS = False  # Set to False to disable saving enhanced documents\n",
    "ENHANCED_DOCUMENTS_FOLDER = './DocumentsStore/output_enhanced'  # Folder to save enhanced documents\n",
    "from pathlib import Path\n",
    "ENHANCED_DOCS_PATH = Path(ENHANCED_DOCUMENTS_FOLDER)\n",
    "ENHANCED_DOCS_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_missing_entities(document):\n",
    "    \"\"\"Find additional entity mentions by string-matching existing mentions.\"\"\"\n",
    "    text = document.get('text', '')\n",
    "    annotation_sets = document.get('annotation_sets', {})\n",
    "    total_new_annotations = {}\n",
    "\n",
    "    for ann_set_name, ann_set in annotation_sets.items():\n",
    "        annotations = ann_set.get('annotations', [])\n",
    "        existing_mentions = defaultdict(list)\n",
    "        existing_positions = set()\n",
    "        for annotation in annotations:\n",
    "            start = annotation.get('start')\n",
    "            end = annotation.get('end')\n",
    "            if start is None or end is None:\n",
    "                continue\n",
    "            mention = text[start:end]\n",
    "            existing_mentions[mention].append((start, end, annotation))\n",
    "            for pos in range(start, end):\n",
    "                existing_positions.add(pos)\n",
    "\n",
    "        new_annotations = []\n",
    "        next_annid = ann_set.get('next_annid', len(annotations) + 1)\n",
    "        for mention, occurrences in existing_mentions.items():\n",
    "            if len(mention.strip()) < MIN_MENTION_LENGTH:\n",
    "                continue\n",
    "            template = occurrences[0][2]\n",
    "            escaped = re.escape(mention)\n",
    "            pattern = r'\\b' + escaped + r'\\b'\n",
    "            flags = 0 if CASE_SENSITIVE else re.IGNORECASE\n",
    "            for m in re.finditer(pattern, text, flags):\n",
    "                s, e = m.start(), m.end()\n",
    "                if any(p in existing_positions for p in range(s, e)):\n",
    "                    continue\n",
    "                new_annotation = {\n",
    "                    'start': s,\n",
    "                    'end': e,\n",
    "                    'type': template.get('type', 'Unknown'),\n",
    "                    'features': template.get('features', {}).copy(),\n",
    "                    'id': next_annid,\n",
    "                }\n",
    "                new_annotations.append(new_annotation)\n",
    "                for p in range(s, e):\n",
    "                    existing_positions.add(p)\n",
    "                next_annid += 1\n",
    "        if new_annotations:\n",
    "            annotations.extend(new_annotations)\n",
    "            ann_set['next_annid'] = next_annid\n",
    "            total_new_annotations[ann_set_name] = len(new_annotations)\n",
    "        else:\n",
    "            total_new_annotations[ann_set_name] = 0\n",
    "\n",
    "    total_added = sum(total_new_annotations.values())\n",
    "    logger.info('Document %s: added %s new entities', document.get('name', 'unknown'), total_added)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_duplicate_annotation_sets(document):\n",
    "    \"\"\"Merge clusters with identical titles (case-insensitive) inside document.features.clusters.\"\"\"\n",
    "    features = document.get('features', {})\n",
    "    clusters = features.get('clusters')\n",
    "    if not clusters or not isinstance(clusters, dict):\n",
    "        logger.debug('No clusters structure found; skipping merge')\n",
    "        return document\n",
    "\n",
    "    total_clusters_merged = 0\n",
    "    total_mentions_moved = 0\n",
    "    for ann_set_name, cluster_list in clusters.items():\n",
    "        if not isinstance(cluster_list, list):\n",
    "            continue\n",
    "        title_map = {}\n",
    "        new_clusters = []\n",
    "        for cluster in cluster_list:\n",
    "            title = cluster.get('title')\n",
    "            if not title:\n",
    "                new_clusters.append(cluster)\n",
    "                continue\n",
    "            key = title.lower().strip()\n",
    "            if key in title_map:\n",
    "                primary = title_map[key]\n",
    "                mentions = cluster.get('mentions', [])\n",
    "                if mentions:\n",
    "                    primary.setdefault('mentions', []).extend(mentions)\n",
    "                    total_mentions_moved += len(mentions)\n",
    "                    total_clusters_merged += 1\n",
    "            else:\n",
    "                title_map[key] = cluster\n",
    "                new_clusters.append(cluster)\n",
    "        clusters[ann_set_name] = new_clusters\n",
    "        logger.info('After merging %s clusters in %s', len(new_clusters), ann_set_name)\n",
    "\n",
    "    if total_clusters_merged:\n",
    "        logger.info('Merged %s clusters and moved %s mentions in total', total_clusters_merged, total_mentions_moved)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from bson import ObjectId\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_for_json(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {key: clean_for_json(value) for key, value in obj.items() if key not in ['_id', 'annotationSetId'] and not key.startswith('__')}\n",
    "    if isinstance(obj, list):\n",
    "        return [clean_for_json(item) for item in obj]\n",
    "    if isinstance(obj, ObjectId):\n",
    "        return str(obj)\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    return obj\n",
    "\n",
    "def save_enhanced_document(document, output_folder: Path):\n",
    "    if not SAVE_ENHANCED_DOCUMENTS:\n",
    "        return\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    doc_name = document.get('name') or ('document_' + get_string_hash(document.get('text', '')[:64]))\n",
    "    safe_name = ''.join(c for c in doc_name if c.isalnum() or c in (' ', '-', '_', '.')).strip() or 'document'\n",
    "    filename = f'{safe_name}.json'\n",
    "    filepath = output_folder / filename\n",
    "    try:\n",
    "        clean_document = clean_for_json(document)\n",
    "        with filepath.open('w', encoding='utf-8') as f:\n",
    "            json.dump(clean_document, f, ensure_ascii=False, indent=2)\n",
    "        logger.info('Enhanced document saved: %s', filename)\n",
    "    except Exception as e:\n",
    "        logger.exception('Error saving enhanced document %s: %s', filename, e)\n",
    "        try:\n",
    "            fallback = output_folder / f'document_{abs(hash(str(document))) % (10**8)}.json'\n",
    "            with fallback.open('w', encoding='utf-8') as f:\n",
    "                json.dump(clean_document, f, ensure_ascii=False, indent=2)\n",
    "            logger.info('Enhanced document saved with fallback name: %s', fallback.name)\n",
    "        except Exception as e2:\n",
    "            logger.exception('Failed to save enhanced document even with fallback: %s', e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mongodb://root:oovailosoozohthu1phoh0eew1oolaePha8xo5kee4iig5@127.0.0.1:27018/\n"
     ]
    }
   ],
   "source": [
    "print(MONGO_URI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Database deletion cancelled\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0a1fb4a8784623970f29e4678db853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get MongoDB credentials from environment variables\n",
    "MONGO_USER = os.environ.get('MONGO_USER')\n",
    "MONGO_PASSWORD = os.environ.get('MONGO_PASSWORD')\n",
    "\n",
    "# Construct Mongo URI using credentials\n",
    "MONGO_URI = f\"mongodb://{MONGO_USER}:{MONGO_PASSWORD}@127.0.0.1:27018/\" if MONGO_USER and MONGO_PASSWORD else None\n",
    "\n",
    "DATABASE_NAME = os.environ.get('MONGO_DB', 'anonymized')\n",
    "DRY_RUN = False\n",
    "\n",
    "# Configuration for database deletion\n",
    "DELETE_EXISTING_DATABASE = True  # Set to True to delete existing database before processing\n",
    "CONFIRM_DELETE = True  # Set to False to skip confirmation prompt\n",
    "\n",
    "client = MongoClient(MONGO_URI) if not DRY_RUN else None\n",
    "db = client[DATABASE_NAME] if client else None\n",
    "\n",
    "if DRY_RUN:\n",
    "    logger.info('DRY_RUN is enabled; no writes to MongoDB will be performed')\n",
    "\n",
    "# Delete existing database if requested\n",
    "if DELETE_EXISTING_DATABASE and not DRY_RUN and client is not None:\n",
    "    if CONFIRM_DELETE:\n",
    "        response = input(f\"Are you sure you want to delete the database '{DATABASE_NAME}'? (yes/no): \")\n",
    "        if response.lower() in ['yes', 'y']:\n",
    "            client.drop_database(DATABASE_NAME)\n",
    "            logger.info(f'Database \"{DATABASE_NAME}\" has been deleted')\n",
    "            # Reconnect to the database (it will be recreated when first used)\n",
    "            db = client[DATABASE_NAME]\n",
    "        else:\n",
    "            logger.info('Database deletion cancelled')\n",
    "    else:\n",
    "        client.drop_database(DATABASE_NAME)\n",
    "        logger.info(f'Database \"{DATABASE_NAME}\" has been deleted automatically')\n",
    "        # Reconnect to the database (it will be recreated when first used)\n",
    "        db = client[DATABASE_NAME]\n",
    "\n",
    "for doc in tqdm(data):\n",
    "    try:\n",
    "        doc = clean_doc(doc)\n",
    "    except Exception as e:\n",
    "        logger.exception('Error cleaning document %s: %s', doc.get('id', 'unknown'), e)\n",
    "        continue\n",
    "\n",
    "    if ENABLE_CLUSTER_MERGING:\n",
    "        try:\n",
    "            doc = merge_duplicate_annotation_sets(doc)\n",
    "        except Exception as e:\n",
    "            logger.exception('Error merging duplicate annotation sets for doc %s: %s', doc.get('id', 'unknown'), e)\n",
    "\n",
    "    if ENABLE_ENTITY_PREPROCESSING:\n",
    "        try:\n",
    "            doc = find_missing_entities(doc)\n",
    "            save_enhanced_document(doc, ENHANCED_DOCS_PATH)\n",
    "        except Exception as e:\n",
    "            logger.exception('Error in entity preprocessing for doc %s: %s', doc.get('id', 'unknown'), e)\n",
    "\n",
    "    try:\n",
    "        if not DRY_RUN and db is not None:\n",
    "            process_doc_for_mongo(doc, db)\n",
    "        else:\n",
    "            # In dry-run mode, just validate the document processing path\n",
    "            logger.debug('Dry-run: would process doc %s', doc.get('id', 'unknown'))\n",
    "    except Exception as e:\n",
    "        logger.exception('Error processing doc to Mongo %s: %s', doc.get('id', 'unknown'), e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
