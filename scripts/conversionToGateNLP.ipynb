{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f04a30ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from gatenlp import Document\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf2e0720",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_doc_dict = defaultdict(str)\n",
    "for file in os.listdir(\"../data\"):\n",
    "    file_path = os.path.join(\"../data\", file)\n",
    "    if os.path.isfile(file_path):        \n",
    "        with open(file_path, 'r') as fileReader:\n",
    "          content = fileReader.read()\n",
    "          text_doc_dict[file] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "758ca877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTERING_ENABLED=True\n"
     ]
    }
   ],
   "source": [
    "# Configuration for clustering\n",
    "ENABLE_CLUSTERING = True  # Set to True to enable entity clustering\n",
    "print(f'CLUSTERING_ENABLED={ENABLE_CLUSTERING}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c545dc65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['GDPR_Chapter_X.txt', 'DataAct_Chapter_VII.txt', 'GDPR_Chapter_VIII.txt', 'AIAct_Chapter_XI.txt', 'GDPR_Chapter_III.txt', 'DataGovernanceAct_Chapter_V.txt', 'DataAct_Chapter_VI.txt', 'AIAct_Chapter_intro.txt', 'GDPR_Chapter_VI.txt', 'GDPR_Chapter_I.txt', 'DataAct_Chapter_VIII.txt', 'DataAct_Chapter_III.txt', 'GDPR_Chapter_VII.txt', 'AIAct_Chapter_XIII.txt', 'AIAct_Chapter_X.txt', 'DataGovernanceAct_Chapter_VIII.txt', 'AIAct_Chapter_VII.txt', 'GDPR_Chapter_II.txt', 'AIAct_Chapter_IV.txt', 'DataAct_Chapter_II.txt', 'DataGovernanceAct_Chapter_IV.txt', 'DataAct_Chapter_V.txt', 'AIAct_Chapter_I.txt', 'AIAct_Chapter_III.txt', 'AIAct_Chapter_VIII.txt', 'GDPR_Chapter_IX.txt', 'DataAct-intro.txt', 'DataAct_Chapter_IX.txt', 'DataGovernanceAct_Chapter_intro.txt', 'DataAct_Chapter_X.txt', 'DataGovernanceAct_Chapter_VII.txt', 'DataGovernanceAct_Chapter_II.txt', 'GDPR_intro.txt', 'DataAct_Chapter_IV.txt', 'AIAct_Chapter_II.txt', 'GDPR_Chapter_IV.txt', 'AIAct_Chapter_V.txt', 'DataGovernanceAct_Chapter_III.txt', 'DataAct_Chapter_I.txt', 'AIAct_Chapter_XII.txt', 'DataGovernanceAct_Chapter_IX.txt', 'AIAct_Chapter_IX.txt', 'DataGovernanceAct_Chapter_I.txt', 'GDPR_Chapter_XI.txt', 'DataAct_Chapter_XI.txt', 'AIAct_Chapter_VI.txt', 'DataGovernanceAct_Chapter_VI.txt', 'GDPR_Chapter_V.txt'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_doc_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a7470987",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdocs = []\n",
    "entities_path = \"../entities\"\n",
    "\n",
    "# Entity type mapping\n",
    "entity_type_mapping = {\n",
    "    \"REGULATION_REF\": \"REGULATION\",\n",
    "    \"ACTOR_ROLE\": \"ROLE\", \n",
    "    \"PROCESSING_OPERATION\": \"DATA.OP\",\n",
    "    \"DATA_CATEGORY\": \"DATA.CAT\"\n",
    "    # AUTHORITY remains unchanged\n",
    "}\n",
    "\n",
    "for key, value in text_doc_dict.items():\n",
    "    base_doc = Document(value)\n",
    "    base_doc.name = key.replace(\".txt\", \"\")\n",
    "    ent_set = base_doc.annset(\"entities_\")\n",
    "    ent_json_path = os.path.join(entities_path, key.replace(\".txt\", \".json\"))\n",
    "    \n",
    "    # Dictionary to store clusters by entity type, then by normalized text\n",
    "    clusters_by_type = {}\n",
    "    cluster_id = 1  # Progressive cluster ID counter\n",
    "    \n",
    "    with open(ent_json_path, \"r\") as json_file:\n",
    "        ent_obj = json.load(json_file)\n",
    "        for ent_type in ent_obj.keys():\n",
    "            # Apply entity type mapping\n",
    "            mapped_ent_type = entity_type_mapping.get(ent_type.upper(), ent_type.upper())\n",
    "            \n",
    "            if ENABLE_CLUSTERING:\n",
    "                clusters_by_type[mapped_ent_type] = {}\n",
    "            \n",
    "            for entity_mention in ent_obj[ent_type]:\n",
    "                # Escape special regex characters in the entity mention\n",
    "                escaped_entity = re.escape(entity_mention)\n",
    "                \n",
    "                # Create a more flexible pattern for entities with special characters\n",
    "                # Use word boundary only at the beginning if it starts with a word character\n",
    "                # Use word boundary only at the end if it ends with a word character\n",
    "                start_boundary = r'\\b' if entity_mention[0].isalnum() else r'(?<!\\w)'\n",
    "                end_boundary = r'\\b' if entity_mention[-1].isalnum() else r'(?!\\w)'\n",
    "                pattern = start_boundary + escaped_entity + end_boundary\n",
    "                \n",
    "                # Find all occurrences using regex\n",
    "                for match in re.finditer(pattern, value, re.IGNORECASE):\n",
    "                    start = match.start()\n",
    "                    end = match.end()\n",
    "                    actual_text = value[start:end]  # Get the actual matched text (preserves case)\n",
    "                    \n",
    "                    # Add annotation with mapped entity type and get the actual annotation object with its ID\n",
    "                    annotation = ent_set.add(start, end, mapped_ent_type, features={\"text\": actual_text})\n",
    "                    actual_annotation_id = annotation.id  # Get the real GateNLP annotation ID\n",
    "                    \n",
    "                    if ENABLE_CLUSTERING:\n",
    "                        # Use the actual matched text for clustering (normalized), only within same type\n",
    "                        normalized_key = actual_text.lower().strip()\n",
    "                        \n",
    "                        if normalized_key not in clusters_by_type[mapped_ent_type]:\n",
    "                            clusters_by_type[mapped_ent_type][normalized_key] = {\n",
    "                                \"id\": cluster_id,  # Unique cluster ID\n",
    "                                \"title\": actual_text,  # Use first occurrence as cluster title\n",
    "                                \"type\": mapped_ent_type,\n",
    "                                \"nelements\": 0,\n",
    "                                \"mentions\": []\n",
    "                            }\n",
    "                            cluster_id += 1  # Increment for next cluster\n",
    "                        \n",
    "                        clusters_by_type[mapped_ent_type][normalized_key][\"mentions\"].append({\n",
    "                            \"id\": actual_annotation_id,  # Use the real GateNLP annotation ID\n",
    "                            \"mention\": actual_text\n",
    "                        })\n",
    "                        clusters_by_type[mapped_ent_type][normalized_key][\"nelements\"] = len(clusters_by_type[mapped_ent_type][normalized_key][\"mentions\"])\n",
    "    \n",
    "    # Add clusters to document features in the correct format (only if clustering is enabled)\n",
    "    if ENABLE_CLUSTERING and clusters_by_type:\n",
    "        all_clusters = []\n",
    "        for ent_type, type_clusters in clusters_by_type.items():\n",
    "            all_clusters.extend(list(type_clusters.values()))\n",
    "        \n",
    "        if all_clusters:\n",
    "            base_doc.features[\"clusters\"] = {\n",
    "                \"entities_\": all_clusters\n",
    "            }\n",
    "    \n",
    "    gdocs.append(base_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44543da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in gdocs:\n",
    "  if not os.path.exists(\"./output\"):\n",
    "    os.mkdir(\"./output\")\n",
    "  file_path = os.path.join(\"./output\", doc.name + '.json')\n",
    "  with open(file_path, 'w') as fileWriter:\n",
    "    json.dump(doc.to_dict(), fileWriter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
