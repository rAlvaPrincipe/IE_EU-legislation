{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elasticsearch Document Processing Pipeline\n",
    "\n",
    "This notebook provides a complete pipeline for processing legal documents and storing them in Elasticsearch with:\n",
    "- Text chunking and vector embeddings\n",
    "- Named Entity Recognition (NER) annotations\n",
    "- Full-text search capabilities\n",
    "- Duplicate detection and removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch Server Version: 8.13.3\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "ELASTICSEARCH_HOST = \"http://localhost:9201\"\n",
    "INDEX_NAME = \"eu_legislation\"\n",
    "JSON_FOLDER = \"./output\"\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 100\n",
    "VECTOR_DIMS = 768\n",
    "MODEL_NAME = \"Alibaba-NLP/gte-multilingual-base\"\n",
    "\n",
    "# Imports\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import torch\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import logging\n",
    "\n",
    "# Initialize Elasticsearch client\n",
    "es = Elasticsearch(ELASTICSEARCH_HOST, verify_certs=False, request_timeout=60)\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    response = es.info()\n",
    "    es_version = response[\"version\"][\"number\"]\n",
    "    print(f\"Connected to Elasticsearch Server Version: {es_version}\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_string_hash(input_string):\n",
    "    \"\"\"Generate SHA256 hash for a given string.\"\"\"\n",
    "    hash_object = hashlib.sha256()\n",
    "    hash_object.update(input_string.encode(\"utf-8\"))\n",
    "    return hash_object.hexdigest()\n",
    "\n",
    "\n",
    "def process_annotation(annotation, text, document_id):\n",
    "    \"\"\"Process a single annotation into the required format.\"\"\"\n",
    "    name = text[annotation[\"start\"]:annotation[\"end\"]]\n",
    "    \n",
    "    ann_object = {\n",
    "        \"mention\": name,\n",
    "        \"start\": annotation[\"start\"],\n",
    "        \"end\": annotation[\"end\"],\n",
    "        \"id\": annotation[\"id\"],\n",
    "        \"type\": annotation[\"type\"],\n",
    "    }\n",
    "    \n",
    "    # Handle linking information\n",
    "    if (\"linking\" in annotation.get(\"features\", {}) and \n",
    "        not annotation[\"features\"][\"linking\"].get(\"is_nil\", True)):\n",
    "        \n",
    "        linking = annotation[\"features\"][\"linking\"]\n",
    "        ann_object.update({\n",
    "            \"display_name\": annotation[\"features\"].get(\"title\", name),\n",
    "            \"is_linked\": True,\n",
    "            \"id_ER\": linking.get(\"top_candidate\", {}).get(\"url\", \"\")\n",
    "        })\n",
    "    else:\n",
    "        ann_object.update({\n",
    "            \"display_name\": name,\n",
    "            \"is_linked\": False,\n",
    "            \"id_ER\": f\"{document_id}_{name}\"\n",
    "        })\n",
    "    \n",
    "    return ann_object\n",
    "\n",
    "\n",
    "def clean_document_data(file_object):\n",
    "    \"\"\"Clean and prepare document data for indexing.\"\"\"\n",
    "    # Remove unnecessary fields\n",
    "    for key in [\"annotation_sets\", \"annoation_sets\", \"features\", \"_id\"]:\n",
    "        if key in file_object:\n",
    "            del file_object[key]\n",
    "    \n",
    "    # Ensure required fields exist\n",
    "    if \"metadata\" not in file_object:\n",
    "        file_object[\"metadata\"] = []\n",
    "    \n",
    "    return file_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample document:\n",
      "{\n",
      "  \"text\": \"CHAPTER I\\nGENERAL PROVISIONS\\nArticle 1\\nSubject matter and scope\\n1. This Regulation lays down harmonised rules, inter alia, on:\\n(a) the making available of product data and related service data to the user of the connected product or related service;\\n(b) the making available of data by data holders to data recipients;\\n(c) the making available of data by data holders to public sector bodies, the Commission, the European Central Bank and Union bodies, where there is an exceptional need for those data for the performance of a specific task carried out in the public interest;\\n(d) facilitating switching between data processing services;\\n(e) introducing safeguards against unlawful third-party access to non-personal data; and\\n(f) the development of interoperability standards for data to be accessed, transferred and used.\\n2. This Regulation covers personal and non-personal data, including the following types of data, in the following contexts:\\n(a) Chapter II applies to data, with the exception of content, concerning the performance, use and environment of connected products and related services;\\n(b) Chapter III applies to any private sector data that is subject to statutory data sharing obligations;\\n(c) Chapter IV applies to any private sector data accessed and used on the basis of contract between enterprises;\\n(d) Chapter V applies to any private sector data with a focus on non-personal data;\\n(e) Chapter VI applies to any data and services processed by providers of data processing services;\\n(f) Chapter VII applies to any non-personal data held in the Union by providers of data processing services.\\n3. This Regulation applies to:\\n(a) manufacturers of connected products placed on the market in the Union and providers of related services, irrespective of the place of establishment of those manufacturers and providers;\\n(b) users in the Union of connected products or related services as referred to in point (a);\\n(c) data holders, irrespective of their place of establishment, that make data available to data recipients in the Union;\\n(d) data recipients in the Union to whom data are made available;\\n22.12.2023 http://data.europa.eu/eli/reg/2023/2854/oj 34/71 Article 2\\n(e) public sector bodies, the Commission, the European Central Bank and Union bodies that request data holders to make data available where there is an exceptional need for those data for the performance of a specific task carried out in the public interest and to the data holders that provide those data in response to such request;\\n(f) providers of data processing services, irrespective of their place of establishment, providing such services to customers in the Union;\\n(g) participants in data spaces and vendors of applications using smart contracts and persons whose trade, business or profession involves the deployment of smart contracts for others in the context of executing an agreement.\\n4. Where this Regulation refers to connected products or related services, such references are also understood to include virtual assistants insofar as they interact with a connected product or related service.\\n5. This Regulation is without prejudice to Union and national law on the protection of personal data, privacy and confidentiality of communications and integrity of terminal equipment, which shall apply to personal data processed in connection with the rights and obligations laid down herein, in particular Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive 2002/58/EC, including the powers and competences of supervisory authorities and the rights of data subjects.\\nInsofar as users are data subjects, the rights laid down in Chapter II of this Regulation shall complement the rights of access by data subjects and rights to data portability under Articles 15 and 20 of Regulation (EU) 2016/679.\\nIn the event of a conflict between this Regulation and Union law on the protection of personal data or privacy, or national legislation adopted in accordance with such Union law, the relevant Union or national law on the protection of personal data or privacy shall prevail.\\n6. This Regulation does not apply to or pre-empt voluntary arrangements for the exchange of data between private and public entities, in particular voluntary arrangements for data sharing.\\nThis Regulation does not affect Union or national legal acts providing for the sharing of, access to and the use of data for the purpose of the prevention, investigation, detection or prosecution of criminal offences or for the execution of criminal penalties, or for customs and taxation purposes, in particular Regulations (EU) 2021/784, (EU) 2022/2065 and (EU) 2023/1543 and Directive (EU) 2023/1544, or international cooperation in that area.\\nThis Regulation does not apply to the collection or sharing of, access to or the use of data under Regulation (EU) 2015/847 and Directive (EU) 2015/849.\\nThis Regulation does not apply to areas that fall outside the scope of Union law and in any event does not affect the competences of the Member States concerning public security, defence or national security, regardless of the type of entity entrusted by the Member States to carry out tasks in relation to those competences, or their power to safeguard other essential State functions, including ensuring the territorial integrity of the State and the maintenance of law and order.\\nThis Regulation does not affect the competences of the Member States concerning customs and tax administration or the health and safety of citizens.\\n7. This Regulation complements the self-regulatory approach of Regulation (EU) 2018/1807 by adding generally applicable obligations on cloud switching.\\n8. This Regulation is without prejudice to Union and national legal acts providing for the protection of intellectual property rights, in particular Directives 2001/29/EC, 2004/48/EC and (EU) 2019/790.\\n9. This Regulation complements and is without prejudice to Union law which aims to promote the interests of consumers and ensure a high level of consumer protection, and to protect their health, safety and economic interests, in particular Directives 93/13/EEC, 2005/29/EC and 2011/83/EU.\\n10.\\nThis Regulation does not preclude the conclusion of voluntary lawful data sharing contracts, including contracts concluded on a reciprocal basis, which comply with the requirements laid down in this Regulation.\\nDefinitions\\nFor the purposes of this Regulation, the following definitions apply:\\n(1) \\u2018data\\u2019 means any digital representation of acts, facts or information and any compilation of such acts, facts or information, including in the form of sound, visual or audio-visual recording;\\n(2) \\u2018metadata\\u2019 means a structured description of the contents or the use of data facilitating the discovery or use of that data;\\n(3) \\u2018personal data\\u2019 means personal data as defined in Article 4, point (1), of Regulation (EU) 2016/679;\\n(4) \\u2018non-personal data\\u2019 means data other than personal data;\\n(5) \\u2018connected product\\u2019 means an item that obtains, generates or collects data concerning its use or environment and that is able to communicate product data via an electronic communications service, physical connection or on-device access, and whose primary function is not the storing, processing or transmission of data on behalf of any party other than the user;\\n(6) \\u2018related service\\u2019 means a digital service, other than an electronic communications service, including software, which is connected with the product at the time of the purchase, rent or lease in such a way that its absence would prevent the connected product from performing one or more of its functions, or which is subsequently connected to the product by the manufacturer or a third party to add to, update or adapt the functions of the connected product;\\n(7) \\u2018processing\\u2019 means any operation or set of operations which is performed on data or on sets of data, whether or not by automated means, such as collection, recording, organisation, structuring, storage, adaptation or alteration, retrieval, consultation, use, disclosure by transmission, dissemination, or other means of making them available, alignment or combination, restriction, erasure or destruction;\\n(8) \\u2018data processing service\\u2019 means a digital service that is provided to a customer and that enables ubiquitous and on-demand network access to a shared pool of configurable, scalable and elastic computing resources of a centralised, distributed or highly distributed nature that can be rapidly provisioned and released with minimal management effort or service provider interaction;\\n(9) \\u2018same service type\\u2019 means a set of data processing services that share the same primary objective, data processing service model and main functionalities;\\n(10) \\u2018data intermediation service\\u2019 means data intermediation service as defined in Article 2, point (11), of Regulation (EU) 2022/868;\\n(11) \\u2018data subject\\u2019 means data subject as referred to in Article 4, point (1), of Regulation (EU) 2016/679;\\n(12) \\u2018user\\u2019 means a natural or legal person that owns a connected product or to whom temporary rights to use that connected product have been contractually transferred, or that receives related services;\\n(13) \\u2018data holder\\u2019 means a natural or legal person that has the right or obligation, in accordance with this Regulation, applicable Union law or national legislation adopted in accordance with Union law, to use and make available data, including, where contractually agreed, product data or related service data which it has retrieved or generated during the provision of a related service;\\n22.12.2023 http://data.europa.eu/eli/reg/2023/2854/oj 36/71 \\u2018customer\\u2019 means a natural or legal person that has entered into a contractual relationship with a provider of data\\n\\u2018data recipient\\u2019 means a natural or legal person, acting for purposes which are related to that person\\u2019s trade, business, craft or profession, other than the user of a connected product or related service, to whom the data holder makes data available, including a third party following a request by the user to the data holder or in accordance with a legal obligation under Union law or national legislation adopted in accordance with Union law;\\n(15) \\u2018product data\\u2019 means data generated by the use of a connected product that the manufacturer designed to be retrievable, via an electronic communications service, physical connection or on-device access, by a user, data holder or a third party, including, where relevant, the manufacturer;\\n(16) \\u2018related service data\\u2019 means data representing the digitisation of user actions or of events related to the connected product, recorded intentionally by the user or generated as a by-product of the user\\u2019s action during the provision of a related service by the provider;\\n(17) \\u2018readily available data\\u2019 means product data and related service data that a data holder lawfully obtains or can lawfully obtain from the connected product or related service, without disproportionate effort going beyond a simple operation;\\n(18) \\u2018trade secret\\u2019 means trade secret as defined in Article 2, point (1), of Directive (EU) 2016/943;\\n(19) \\u2018trade secret holder\\u2019 means a trade secret holder as defined in Article 2, point (2), of Directive (EU) 2016/943;\\n(20) \\u2018profiling\\u2019 means profiling as defined in Article 4, point (4), of Regulation (EU) 2016/679;\\n(21) \\u2018making available on the market\\u2019 means any supply of a connected product for distribution, consumption or use on the Union market in the course of a commercial activity, whether in return for payment or free of charge;\\n(22) \\u2018placing on the market\\u2019 means the first making available of a connected product on the Union market;\\n(23) \\u2018consumer\\u2019 means any natural person who is acting for purposes which are outside that person\\u2019s trade, business, craft or profession;\\n(24) \\u2018enterprise\\u2019 means a natural or legal person that, in relation to contracts and practices covered by this Regulation, is acting for purposes which are related to that person\\u2019s trade, business, craft or profession;\\n(25) \\u2018small enterprise\\u2019 means a small enterprise as defined in Article 2(2) of the Annex to Recommendation 2003/361/EC;\\n(26) \\u2018microenterprise\\u2019 means a microenterprise as defined in Article 2(3) of the Annex to Recommendation 2003/361/EC;\\n(27) \\u2018Union bodies\\u2019 means the Union bodies, offices and agencies set up by or pursuant to acts adopted on the basis of the Treaty on European Union, the TFEU or the Treaty establishing the European Atomic Energy Community;\\n(28) \\u2018public sector body\\u2019 means national, regional or local authorities of the Member States and bodies governed by public law of the Member States, or associations formed by one or more such authorities or one or more such bodies;\\n(29) \\u2018public emergency\\u2019 means an exceptional situation, limited in time, such as a public health emergency, an emergency resulting from natural disasters, a human-induced major disaster, including a major cybersecurity incident, negatively affecting the population of the Union or the whole or part of a Member State, with a risk of serious and lasting repercussions for living conditions or economic stability, financial stability, or the substantial and immediate degradation of economic assets in the Union or the relevant Member State and which is determined or officially declared in accordance with the relevant procedures under Union or national law;\\nprocessing services with the objective of using one or more data processing services;\\n(31) \\u2018virtual assistants\\u2019 means software that can process demands, tasks or questions including those based on audio, written input, gestures or motions, and that, based on those demands, tasks or questions, provides access to other services or controls the functions of connected products;\\n(32) \\u2018digital assets\\u2019 means elements in digital form, including applications, for which the customer has the right of use, independently from the contractual relationship with the data processing service it intends to switch from;\\n(33) \\u2018on-premises ICT infrastructure\\u2019 means ICT infrastructure and computing resources owned, rented or leased by the customer, located in the data centre of the customer itself and operated by the customer or by a third-party;\\n(34) \\u2018switching\\u2019 means the process involving a source provider of data processing services, a customer of a data processing service and, where relevant, a destination provider of data processing services, whereby the customer of a data processing service changes from using one data processing service to using another data processing service of the same service type, or other service, offered by a different provider of data processing services, or to an on-premises ICT infrastructure, including through extracting, transforming and uploading the data;\\n(35) \\u2018data egress charges\\u2019 means data transfer fees charged to customers for extracting their data through the network from the ICT infrastructure of a provider of data processing services to the system of a different provider or to on-premises ICT infrastructure;\\n(36) \\u2018switching charges\\u2019 means charges, other than standard service fees or early termination penalties, imposed by a provider of data processing services on a customer for the actions mandated by this Regulation for switching to the system of a different provider or to on-premises ICT infrastructure, including data egress charges;\\n(37) \\u2018functional equivalence\\u2019 means re-establishing on the basis of the customer\\u2019s exportable data and digital assets, a minimum level of functionality in the environment of a new data processing service of the same service type after the switching process, where the destination data processing service delivers a materially comparable outcome in response to the same input for shared features supplied to the customer under the contract;\\n(38) \\u2018exportable data\\u2019, for the purpose of Articles 23 to 31 and Article 35, means the input and output data, including metadata, directly or indirectly generated, or cogenerated, by the customer\\u2019s use of the data processing service, excluding any assets or data protected by intellectual property rights, or constituting a trade secret, of providers of data processing services or third parties;\\n(39) \\u2018smart contract\\u2019 means a computer program used for the automated execution of an agreement or part thereof, using a sequence of electronic data records and ensuring their integrity and the accuracy of their chronological ordering;\\n(40) \\u2018interoperability\\u2019 means the ability of two or more data spaces or communication networks, systems, connected products, applications, data processing services or components to exchange and use data in order to perform their functions;\\nopen interoperability specification\\u2019 means a technical specification in the field of information and communication technologies which is performance oriented towards achieving interoperability between data processing services;\\n22.12.2023 http://data.europa.eu/eli/reg/2023/2854/oj 38/71\\n\\u2018common specifications\\u2019 means a document, other than a standard, containing technical solutions providing a means to comply with certain requirements and obligations established under this Regulation;\\n(43) \\u2018harmonised standard\\u2019 means a harmonised standard as defined in Article 2, point (1)(c), of Regulation (EU) No 1025/2012.\\nDefinitions\\nFor the purposes of this Regulation, the following definitions apply:\\n(1) \\u2018data\\u2019 means any digital representation of acts, facts or information and any compilation of such acts, facts or information, including in the form of sound, visual or audio-visual recording;\\n(2) \\u2018metadata\\u2019 means a structured description of the contents or the use of data facilitating the discovery or use of that data;\\n(3) \\u2018personal data\\u2019 means personal data as defined in Article 4, point (1), of Regulation (EU) 2016/679;\\n(4) \\u2018non-personal data\\u2019 means data other than personal data;\\n(5) \\u2018connected product\\u2019 means an item that obtains, generates or collects data concerning its use or environment and that is able to communicate product data via an electronic communications service, physical connection or on-device access, and whose primary function is not the storing, processing or transmission of data on behalf of any party other than the user;\\n(6) \\u2018related service\\u2019 means a digital service, other than an electronic communications service, including software, which is connected with the product at the time of the purchase, rent or lease in such a way that its absence would prevent the connected product from performing one or more of its functions, or which is subsequently connected to the product by the manufacturer or a third party to add to, update or adapt the functions of the connected product;\\n(7) \\u2018processing\\u2019 means any operation or set of operations which is performed on data or on sets of data, whether or not by automated means, such as collection, recording, organisation, structuring, storage, adaptation or alteration, retrieval, consultation, use, disclosure by transmission, dissemination, or other means of making them available, alignment or combination, restriction, erasure or destruction;\\n(8) \\u2018data processing service\\u2019 means a digital service that is provided to a customer and that enables ubiquitous and on-demand network access to a shared pool of configurable, scalable and elastic computing resources of a centralised, distributed or highly distributed nature that can be rapidly provisioned and released with minimal management effort or service provider interaction;\\n(9) \\u2018same service type\\u2019 means a set of data processing services that share the same primary objective, data processing service model and main functionalities;\\n(10) \\u2018data intermediation service\\u2019 means data intermediation service as defined in Article 2, point (11), of Regulation (EU) 2022/868;\\n(11) \\u2018data subject\\u2019 means data subject as referred to in Article 4, point (1), of Regulation (EU) 2016/679;\\n(12) \\u2018user\\u2019 means a natural or legal person that owns a connected product or to whom temporary rights to use that connected product have been contractually transferred, or that receives related services;\\n(13) \\u2018data holder\\u2019 means a natural or legal person that has the right or obligation, in accordance with this Regulation, applicable Union law or national legislation adopted in accordance with Union law, to use and make available data, including, where contractually agreed, product data or related service data which it has retrieved or generated during the provision of a related service;\\n22.12.2023 http://data.europa.eu/eli/reg/2023/2854/oj 36/71 \\u2018customer\\u2019 means a natural or legal person that has entered into a contractual relationship with a provider of data\\n\\u2018data recipient\\u2019 means a natural or legal person, acting for purposes which are related to that person\\u2019s trade, business, craft or profession, other than the user of a connected product or related service, to whom the data holder makes data available, including a third party following a request by the user to the data holder or in accordance with a legal obligation under Union law or national legislation adopted in accordance with Union law;\\n(15) \\u2018product data\\u2019 means data generated by the use of a connected product that the manufacturer designed to be retrievable, via an electronic communications service, physical connection or on-device access, by a user, data holder or a third party, including, where relevant, the manufacturer;\\n(16) \\u2018related service data\\u2019 means data representing the digitisation of user actions or of events related to the connected product, recorded intentionally by the user or generated as a by-product of the user\\u2019s action during the provision of a related service by the provider;\\n(17) \\u2018readily available data\\u2019 means product data and related service data that a data holder lawfully obtains or can lawfully obtain from the connected product or related service, without disproportionate effort going beyond a simple operation;\\n(18) \\u2018trade secret\\u2019 means trade secret as defined in Article 2, point (1), of Directive (EU) 2016/943;\\n(19) \\u2018trade secret holder\\u2019 means a trade secret holder as defined in Article 2, point (2), of Directive (EU) 2016/943;\\n(20) \\u2018profiling\\u2019 means profiling as defined in Article 4, point (4), of Regulation (EU) 2016/679;\\n(21) \\u2018making available on the market\\u2019 means any supply of a connected product for distribution, consumption or use on the Union market in the course of a commercial activity, whether in return for payment or free of charge;\\n(22) \\u2018placing on the market\\u2019 means the first making available of a connected product on the Union market;\\n(23) \\u2018consumer\\u2019 means any natural person who is acting for purposes which are outside that person\\u2019s trade, business, craft or profession;\\n(24) \\u2018enterprise\\u2019 means a natural or legal person that, in relation to contracts and practices covered by this Regulation, is acting for purposes which are related to that person\\u2019s trade, business, craft or profession;\\n(25) \\u2018small enterprise\\u2019 means a small enterprise as defined in Article 2(2) of the Annex to Recommendation 2003/361/EC;\\n(26) \\u2018microenterprise\\u2019 means a microenterprise as defined in Article 2(3) of the Annex to Recommendation 2003/361/EC;\\n(27) \\u2018Union bodies\\u2019 means the Union bodies, offices and agencies set up by or pursuant to acts adopted on the basis of the Treaty on European Union, the TFEU or the Treaty establishing the European Atomic Energy Community;\\n(28) \\u2018public sector body\\u2019 means national, regional or local authorities of the Member States and bodies governed by public law of the Member States, or associations formed by one or more such authorities or one or more such bodies;\\n(29) \\u2018public emergency\\u2019 means an exceptional situation, limited in time, such as a public health emergency, an emergency resulting from natural disasters, a human-induced major disaster, including a major cybersecurity incident, negatively affecting the population of the Union or the whole or part of a Member State, with a risk of serious and lasting repercussions for living conditions or economic stability, financial stability, or the substantial and immediate degradation of economic assets in the Union or the relevant Member State and which is determined or officially declared in accordance with the relevant procedures under Union or national law;\\nprocessing services with the objective of using one or more data processing services;\\n(31) \\u2018virtual assistants\\u2019 means software that can process demands, tasks or questions including those based on audio, written input, gestures or motions, and that, based on those demands, tasks or questions, provides access to other services or controls the functions of connected products;\\n(32) \\u2018digital assets\\u2019 means elements in digital form, including applications, for which the customer has the right of use, independently from the contractual relationship with the data processing service it intends to switch from;\\n(33) \\u2018on-premises ICT infrastructure\\u2019 means ICT infrastructure and computing resources owned, rented or leased by the customer, located in the data centre of the customer itself and operated by the customer or by a third-party;\\n(34) \\u2018switching\\u2019 means the process involving a source provider of data processing services, a customer of a data processing service and, where relevant, a destination provider of data processing services, whereby the customer of a data processing service changes from using one data processing service to using another data processing service of the same service type, or other service, offered by a different provider of data processing services, or to an on-premises ICT infrastructure, including through extracting, transforming and uploading the data;\\n(35) \\u2018data egress charges\\u2019 means data transfer fees charged to customers for extracting their data through the network from the ICT infrastructure of a provider of data processing services to the system of a different provider or to on-premises ICT infrastructure;\\n(36) \\u2018switching charges\\u2019 means charges, other than standard service fees or early termination penalties, imposed by a provider of data processing services on a customer for the actions mandated by this Regulation for switching to the system of a different provider or to on-premises ICT infrastructure, including data egress charges;\\n(37) \\u2018functional equivalence\\u2019 means re-establishing on the basis of the customer\\u2019s exportable data and digital assets, a minimum level of functionality in the environment of a new data processing service of the same service type after the switching process, where the destination data processing service delivers a materially comparable outcome in response to the same input for shared features supplied to the customer under the contract;\\n(38) \\u2018exportable data\\u2019, for the purpose of Articles 23 to 31 and Article 35, means the input and output data, including metadata, directly or indirectly generated, or cogenerated, by the customer\\u2019s use of the data processing service, excluding any assets or data protected by intellectual property rights, or constituting a trade secret, of providers of data processing services or third parties;\\n(39) \\u2018smart contract\\u2019 means a computer program used for the automated execution of an agreement or part thereof, using a sequence of electronic data records and ensuring their integrity and the accuracy of their chronological ordering;\\n(40) \\u2018interoperability\\u2019 means the ability of two or more data spaces or communication networks, systems, connected products, applications, data processing services or components to exchange and use data in order to perform their functions;\\nopen interoperability specification\\u2019 means a technical specification in the field of information and communication technologies which is performance oriented towards achieving interoperability between data processing services;\\n22.12.2023 http://data.europa.eu/eli/reg/2023/2854/oj 38/71\\n\\u2018common specifications\\u2019 means a document, other than a standard, containing technical solutions providing a means to comply with certain requirements and obligations established under this Regulation;\\n(43) \\u2018harmonised standard\\u2019 means a harmonised standard as defined in Article 2, point (1)(c), of Regulation (EU) No 1025/2012.\\n\",\n",
      "  \"offset_type\": \"p\",\n",
      "  \"name\": \"DataAct_Chapter_I\",\n",
      "  \"id\": \"1d31d8a863a967407b6332fc83a0314a3788a7627617f4fe033b614691c05279\",\n",
      "  \"metadata\": []\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d4/l_fskgld3vddb2mq0h787wd40000gn/T/ipykernel_12536/352670684.py:13: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  return es.search(index=INDEX_NAME, body=search_query)\n"
     ]
    }
   ],
   "source": [
    "# Quick document search and inspection\n",
    "def search_documents(query=\"*\", size=10, exclude_fields=None):\n",
    "    \"\"\"Search documents in the index with optional field exclusions.\"\"\"\n",
    "    if exclude_fields is None:\n",
    "        exclude_fields = [\"chunks\", \"annotations\"]\n",
    "    \n",
    "    search_query = {\n",
    "        \"query\": {\"query_string\": {\"query\": query}},\n",
    "        \"_source\": {\"excludes\": exclude_fields},\n",
    "        \"size\": size,\n",
    "    }\n",
    "    \n",
    "    return es.search(index=INDEX_NAME, body=search_query)\n",
    "\n",
    "\n",
    "def find_empty_annotation_documents():\n",
    "    \"\"\"Find documents with empty annotations field.\"\"\"\n",
    "    search_query = {\n",
    "        \"query\": {\"query_string\": {\"query\": \"*\"}},\n",
    "        \"_source\": {\"excludes\": [\"chunks\", \"annotation_sets\"]},\n",
    "    }\n",
    "    \n",
    "    response = es.search(index=INDEX_NAME, body=search_query)\n",
    "    empty_annotation_ids = []\n",
    "    \n",
    "    for hit in response[\"hits\"][\"hits\"]:\n",
    "        if hit[\"_source\"].get(\"annotations\") == []:\n",
    "            name = hit[\"_source\"].get(\"name\", \"(no name)\")\n",
    "            print(f\"Document with empty 'annotations' field: {name}\")\n",
    "            empty_annotation_ids.append(hit[\"_source\"][\"id\"])\n",
    "    \n",
    "    return empty_annotation_ids\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sample_response = search_documents(size=1)\n",
    "if sample_response[\"hits\"][\"hits\"]:\n",
    "    print(\"Sample document:\")\n",
    "    print(json.dumps(sample_response[\"hits\"][\"hits\"][0][\"_source\"], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing index: eu_legislation\n",
      "Created index: eu_legislation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d4/l_fskgld3vddb2mq0h787wd40000gn/T/ipykernel_12536/1619132199.py:65: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  response = es.indices.create(index=index_name, body=index_settings)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'eu_legislation'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_index_settings():\n",
    "    \"\"\"Get the index settings with custom nested object limit.\"\"\"\n",
    "    return {\n",
    "        \"settings\": {\n",
    "            \"index.mapping.nested_objects.limit\": 20000\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"text\": {\"type\": \"text\"},\n",
    "                \"name\": {\"type\": \"keyword\"},\n",
    "                \"preview\": {\"type\": \"keyword\"},\n",
    "                \"id\": {\"type\": \"keyword\"},\n",
    "                \"metadata\": {\n",
    "                    \"type\": \"nested\",\n",
    "                    \"properties\": {\n",
    "                        \"type\": {\"type\": \"keyword\"},\n",
    "                        \"value\": {\"type\": \"keyword\"}\n",
    "                    }\n",
    "                },\n",
    "                \"annotations\": {\n",
    "                    \"type\": \"nested\",\n",
    "                    \"properties\": {\n",
    "                        \"mention\": {\"type\": \"keyword\"},\n",
    "                        \"start\": {\"type\": \"integer\"},\n",
    "                        \"end\": {\"type\": \"integer\"},\n",
    "                        \"display_name\": {\"type\": \"keyword\"},\n",
    "                        \"id\": {\"type\": \"integer\"},\n",
    "                        \"type\": {\"type\": \"keyword\"},\n",
    "                        \"is_linked\": {\"type\": \"boolean\"},\n",
    "                        \"id_ER\": {\"type\": \"keyword\"}\n",
    "                    }\n",
    "                },\n",
    "                \"chunks\": {\n",
    "                    \"type\": \"nested\",\n",
    "                    \"properties\": {\n",
    "                        \"vectors\": {\n",
    "                            \"type\": \"nested\",\n",
    "                            \"properties\": {\n",
    "                                \"predicted_value\": {\n",
    "                                    \"type\": \"dense_vector\",\n",
    "                                    \"index\": True,\n",
    "                                    \"dims\": VECTOR_DIMS,\n",
    "                                    \"similarity\": \"cosine\",\n",
    "                                },\n",
    "                                \"text\": {\"type\": \"text\"},\n",
    "                                \"entities\": {\"type\": \"text\"},\n",
    "                            },\n",
    "                        },\n",
    "                    },\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def recreate_index(index_name=INDEX_NAME, delete_existing=False):\n",
    "    \"\"\"Create or recreate the Elasticsearch index.\"\"\"\n",
    "    try:\n",
    "        if delete_existing and es.indices.exists(index=index_name):\n",
    "            es.indices.delete(index=index_name)\n",
    "            print(f\"Deleted existing index: {index_name}\")\n",
    "        \n",
    "        if not es.indices.exists(index=index_name):\n",
    "            index_settings = get_index_settings()\n",
    "            response = es.indices.create(index=index_name, body=index_settings)\n",
    "            print(f\"Created index: {index_name}\")\n",
    "            return response\n",
    "        else:\n",
    "            print(f\"Index {index_name} already exists\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error managing index: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Uncomment the line below to recreate the index\n",
    "recreate_index(delete_existing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_files(path, target_ids=None):\n",
    "    \"\"\"\n",
    "    Read and process JSON annotation files from a directory.\n",
    "    \n",
    "    Args:\n",
    "        path: Directory containing .json.annotated files\n",
    "        target_ids: Optional set of document IDs to filter by\n",
    "        \n",
    "    Returns:\n",
    "        List of processed document objects\n",
    "    \"\"\"\n",
    "    json_files = [f for f in os.listdir(path) if f.endswith(\".json\")]\n",
    "    data = []\n",
    "    \n",
    "    print(f\"Processing {len(json_files)} JSON files from {path}\")\n",
    "    \n",
    "    for json_file in tqdm(json_files, desc=\"Reading files\"):\n",
    "        try:\n",
    "            with open(os.path.join(path, json_file), \"r\") as file:\n",
    "                file_object = json.load(file)\n",
    "                \n",
    "                # Skip empty files\n",
    "                if not file_object.get(\"text\"):\n",
    "                    print(f\"Warning: Skipping empty file: {json_file}\")\n",
    "                    continue\n",
    "                \n",
    "                # Generate document ID\n",
    "                file_object[\"id\"] = get_string_hash(file_object[\"text\"])\n",
    "                \n",
    "                # Filter by target IDs if provided\n",
    "                if target_ids and file_object[\"id\"] not in target_ids:\n",
    "                    continue\n",
    "                \n",
    "                # Process annotations\n",
    "                annotations = process_document_annotations(file_object)\n",
    "                file_object[\"annotations\"] = annotations\n",
    "                \n",
    "                # Clean up the document\n",
    "                file_object = clean_document_data(file_object)\n",
    "                \n",
    "                data.append(file_object)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {json_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Successfully processed {len(data)} documents\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def process_document_annotations(file_object):\n",
    "    \"\"\"Extract and process annotations from a document.\"\"\"\n",
    "    text = file_object.get(\"text\", \"\")\n",
    "    annotations = []\n",
    "    \n",
    "    annotation_sets = file_object.get(\"annotation_sets\", {})\n",
    "    entities = annotation_sets.get(\"entities_\", {})\n",
    "    raw_annotations = entities.get(\"annotations\", [])\n",
    "    \n",
    "    for annotation in raw_annotations:\n",
    "        try:\n",
    "            ann_object = process_annotation(annotation, text, file_object.get(\"id\", \"\"))\n",
    "            annotations.append(ann_object)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error processing annotation: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Processed {len(annotations)} annotations for document '{file_object.get('name', 'Unknown')}'\")\n",
    "    return annotations\n",
    "\n",
    "\n",
    "def send_to_elasticsearch(data, index_name=INDEX_NAME, update_existing=True):\n",
    "    \"\"\"\n",
    "    Send documents to Elasticsearch with optional duplicate handling.\n",
    "    \n",
    "    Args:\n",
    "        data: List of document objects\n",
    "        index_name: Target index name\n",
    "        update_existing: Whether to update existing documents\n",
    "    \"\"\"\n",
    "    print(f\"Sending {len(data)} documents to Elasticsearch...\")\n",
    "    \n",
    "    for item in tqdm(data, desc=\"Indexing documents\"):\n",
    "        try:\n",
    "            if update_existing:\n",
    "                # Remove existing documents with same ID\n",
    "                search_query = {\"query\": {\"term\": {\"id\": item[\"id\"]}}}\n",
    "                search_response = es.search(index=index_name, body=search_query)\n",
    "                \n",
    "                for hit in search_response[\"hits\"][\"hits\"]:\n",
    "                    es.delete(index=index_name, id=hit[\"_id\"])\n",
    "            \n",
    "            # Index the new document\n",
    "            es.index(index=index_name, body=item)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error indexing document {item.get('name', 'Unknown')}: {e}\")\n",
    "    \n",
    "    print(\"Document indexing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 48 JSON files from ./output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading files:   0%|          | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading files: 100%|██████████| 48/48 [00:00<00:00, 813.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 122 annotations for document 'DataAct_Chapter_IX'\n",
      "Processed 158 annotations for document 'DataGovernanceAct_Chapter_II'\n",
      "Processed 134 annotations for document 'DataAct_Chapter_VIII'\n",
      "Processed 49 annotations for document 'AIAct_Chapter_XI'\n",
      "Processed 631 annotations for document 'AIAct_Chapter_III'\n",
      "Processed 183 annotations for document 'DataGovernanceAct_Chapter_III'\n",
      "Processed 127 annotations for document 'AIAct_Chapter_I'\n",
      "Processed 67 annotations for document 'GDPR_Chapter_IX'\n",
      "Processed 88 annotations for document 'DataGovernanceAct_Chapter_I'\n",
      "Processed 59 annotations for document 'DataGovernanceAct_Chapter_VI'\n",
      "Processed 137 annotations for document 'DataGovernanceAct_Chapter_IV'\n",
      "Processed 104 annotations for document 'DataAct_Chapter_XI'\n",
      "Processed 204 annotations for document 'DataAct_Chapter_III'\n",
      "Processed 664 annotations for document 'DataAct-intro'\n",
      "Processed 501 annotations for document 'AIAct_Chapter_IX'\n",
      "Processed 24 annotations for document 'GDPR_Chapter_X'\n",
      "Processed 221 annotations for document 'AIAct_Chapter_XIII'\n",
      "Processed 312 annotations for document 'GDPR_Chapter_III'\n",
      "Processed 5 annotations for document 'DataAct_Chapter_X'\n",
      "Processed 59 annotations for document 'GDPR_Chapter_XI'\n",
      "Processed 28 annotations for document 'DataGovernanceAct_Chapter_V'\n",
      "Processed 153 annotations for document 'AIAct_Chapter_V'\n",
      "Processed 30 annotations for document 'DataGovernanceAct_Chapter_VIII'\n",
      "Processed 25 annotations for document 'AIAct_Chapter_IV'\n",
      "Processed 398 annotations for document 'AIAct_Chapter_intro'\n",
      "Processed 42 annotations for document 'DataAct_Chapter_VII'\n",
      "Processed 194 annotations for document 'GDPR_Chapter_VI'\n",
      "Processed 34 annotations for document 'DataGovernanceAct_Chapter_IX'\n",
      "Processed 589 annotations for document 'DataAct_Chapter_II'\n",
      "Processed 113 annotations for document 'GDPR_Chapter_VIII'\n",
      "Processed 80 annotations for document 'GDPR_Chapter_II'\n",
      "Processed 29 annotations for document 'AIAct_Chapter_X'\n",
      "Processed 293 annotations for document 'GDPR_Chapter_VII'\n",
      "Processed 140 annotations for document 'DataAct_Chapter_VI'\n",
      "Processed 553 annotations for document 'DataAct_Chapter_V'\n",
      "Processed 150 annotations for document 'GDPR_Chapter_V'\n",
      "Processed 74 annotations for document 'AIAct_Chapter_II'\n",
      "Processed 34 annotations for document 'DataAct_Chapter_IV'\n",
      "Processed 125 annotations for document 'AIAct_Chapter_VII'\n",
      "Processed 7 annotations for document 'GDPR_intro'\n",
      "Processed 162 annotations for document 'AIAct_Chapter_XII'\n",
      "Processed 40 annotations for document 'DataGovernanceAct_Chapter_VII'\n",
      "Processed 29 annotations for document 'AIAct_Chapter_VIII'\n",
      "Processed 466 annotations for document 'GDPR_Chapter_IV'\n",
      "Processed 156 annotations for document 'GDPR_Chapter_I'\n",
      "Processed 537 annotations for document 'AIAct_Chapter_VI'\n",
      "Processed 501 annotations for document 'DataGovernanceAct_Chapter_intro'\n",
      "Processed 115 annotations for document 'DataAct_Chapter_I'\n",
      "Successfully processed 48 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process documents - you can filter by specific document IDs if needed\n",
    "# empty_annotation_ids = find_empty_annotation_documents()  # Uncomment to filter\n",
    "target_ids = None  # or set to empty_annotation_ids to process only those documents\n",
    "\n",
    "data = read_json_files(JSON_FOLDER, target_ids=target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded {len(data)} documents ready for processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending 48 documents to Elasticsearch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing documents:   0%|          | 0/48 [00:00<?, ?it/s]/var/folders/d4/l_fskgld3vddb2mq0h787wd40000gn/T/ipykernel_12536/2021533814.py:88: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  search_response = es.search(index=index_name, body=search_query)\n",
      "/var/folders/d4/l_fskgld3vddb2mq0h787wd40000gn/T/ipykernel_12536/2021533814.py:94: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use the 'document' parameter. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  es.index(index=index_name, body=item)\n",
      "Indexing documents: 100%|██████████| 48/48 [00:03<00:00, 13.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document indexing completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "send_to_elasticsearch(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Chunking and Vector Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping for chunk vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This mapping update is now included in the main index creation\n",
    "# No need to run separately if using recreate_index() function\n",
    "print(\"Vector mapping is included in the main index configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: Alibaba-NLP/gte-multilingual-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "class DocumentChunker:\n",
    "    \"\"\"Handles document chunking and embedding generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=MODEL_NAME, device=\"mps\"):\n",
    "        \"\"\"Initialize the chunker with sentence transformer model.\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self._model = None\n",
    "        self._initialize_model()\n",
    "    \n",
    "    def _initialize_model(self):\n",
    "        \"\"\"Lazy initialization of the sentence transformer model.\"\"\"\n",
    "        if self._model is None:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self._model = SentenceTransformer(\n",
    "                self.model_name, \n",
    "                trust_remote_code=True\n",
    "            ).to(self.device)\n",
    "            print(\"Model loaded successfully\")\n",
    "    \n",
    "    def generate_chunks_with_embeddings(self, text):\n",
    "        \"\"\"\n",
    "        Split text into chunks and generate embeddings for each chunk.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to chunk and embed\n",
    "            \n",
    "        Returns:\n",
    "            List of lists: [embedding, chunk_text, entities_placeholder]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Split text into chunks\n",
    "            chunks = text_splitter.split_text(text)\n",
    "            \n",
    "            if not chunks:\n",
    "                print(\"Warning: No chunks generated from text\")\n",
    "                return []\n",
    "            \n",
    "            # Generate embeddings\n",
    "            embeddings = self._model.encode(chunks, show_progress_bar=False)\n",
    "            \n",
    "            # Return as list of lists (mutable) instead of tuples (immutable)\n",
    "            result = [[emb.tolist(), chunk, \"\"] for emb, chunk in zip(embeddings, chunks)]\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in chunking/embedding: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_entities_from_chunk(self, chunk_text, full_text, annotations):\n",
    "        \"\"\"\n",
    "        Find entities from annotations that are present in the chunk text.\n",
    "        \n",
    "        Args:\n",
    "            chunk_text: The text of the current chunk\n",
    "            full_text: The full document text \n",
    "            annotations: List of annotation objects\n",
    "        \n",
    "        Returns:\n",
    "            String of entity mentions found in the chunk\n",
    "        \"\"\"\n",
    "        chunk_entities = []\n",
    "        \n",
    "        # Find the position of this chunk in the full text\n",
    "        chunk_start_in_full = full_text.find(chunk_text)\n",
    "        if chunk_start_in_full == -1:\n",
    "            return \"\"\n",
    "        \n",
    "        chunk_end_in_full = chunk_start_in_full + len(chunk_text)\n",
    "        \n",
    "        # Check which annotations overlap with this chunk\n",
    "        for annotation in annotations:\n",
    "            ann_start = annotation.get(\"start\", 0)\n",
    "            ann_end = annotation.get(\"end\", 0)\n",
    "            \n",
    "            # Check if annotation overlaps with chunk boundaries\n",
    "            if ((ann_start >= chunk_start_in_full and ann_start < chunk_end_in_full) or \n",
    "                (ann_end > chunk_start_in_full and ann_end <= chunk_end_in_full) or \n",
    "                (ann_start <= chunk_start_in_full and ann_end >= chunk_end_in_full)):\n",
    "                \n",
    "                entity_mention = full_text[ann_start:ann_end]\n",
    "                chunk_entities.append(entity_mention)\n",
    "        \n",
    "        return \" \".join(chunk_entities)\n",
    "\n",
    "\n",
    "# Initialize the chunker\n",
    "chunker = DocumentChunker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for documents to process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d4/l_fskgld3vddb2mq0h787wd40000gn/T/ipykernel_12536/3966881953.py:65: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  response = es.search(index=index_name, body=search_body, size=batch_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48 documents to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:   0%|          | 0/48 [00:00<?, ?it/s]/var/folders/d4/l_fskgld3vddb2mq0h787wd40000gn/T/ipykernel_12536/3966881953.py:43: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  response = es.update(index=INDEX_NAME, id=doc_id, body=data)\n",
      "Adding chunks and embeddings:   2%|▏         | 1/48 [00:02<01:50,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document OzmPuZkB6MMnAM0IQDdl with 78 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:   4%|▍         | 2/48 [00:09<03:54,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document OjmPuZkB6MMnAM0IQDcY with 255 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:   6%|▋         | 3/48 [00:14<03:47,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document OTmPuZkB6MMnAM0IPzfH with 168 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:   8%|▊         | 4/48 [00:15<02:33,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document ODmPuZkB6MMnAM0IPzd6 with 29 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  10%|█         | 5/48 [00:19<02:40,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document NzmPuZkB6MMnAM0IPzcx with 120 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  12%|█▎        | 6/48 [00:19<01:48,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document NjmPuZkB6MMnAM0IPjfs with 6 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  15%|█▍        | 7/48 [00:20<01:18,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document NTmPuZkB6MMnAM0IPjeu with 12 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  17%|█▋        | 8/48 [00:22<01:13,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document NDmPuZkB6MMnAM0IPjdk with 48 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  19%|█▉        | 9/48 [00:22<00:52,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document MzmPuZkB6MMnAM0IPjck with 3 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  21%|██        | 10/48 [00:24<00:57,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document MjmPuZkB6MMnAM0IPTfj with 68 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  23%|██▎       | 11/48 [00:25<00:47,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document MTmPuZkB6MMnAM0IPTeg with 15 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  25%|██▌       | 12/48 [00:26<00:44,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document MDmPuZkB6MMnAM0IPTdg with 34 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  27%|██▋       | 13/48 [00:27<00:46,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document LzmPuZkB6MMnAM0IPTcd with 49 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  29%|██▉       | 14/48 [00:30<01:00,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document LjmPuZkB6MMnAM0IPDfV with 105 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  31%|███▏      | 15/48 [00:31<00:54,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document LTmPuZkB6MMnAM0IPDeN with 44 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  33%|███▎      | 16/48 [00:34<01:02,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document LDmPuZkB6MMnAM0IPDdH with 83 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  35%|███▌      | 17/48 [00:35<00:47,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document KzmPuZkB6MMnAM0IPDcC with 16 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  38%|███▊      | 18/48 [00:36<00:44,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document KjmPuZkB6MMnAM0IOzfB with 37 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  40%|███▉      | 19/48 [00:37<00:41,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document KTmPuZkB6MMnAM0IOzeB with 33 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  42%|████▏     | 20/48 [00:40<00:52,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document KDmPuZkB6MMnAM0IOzc3 with 113 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  44%|████▍     | 21/48 [00:41<00:41,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document JzmPuZkB6MMnAM0IOjfw with 20 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  46%|████▌     | 22/48 [00:43<00:40,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document JjmPuZkB6MMnAM0IOjes with 44 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  48%|████▊     | 23/48 [00:43<00:30,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document JTmPuZkB6MMnAM0IOjdp with 13 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  50%|█████     | 24/48 [01:01<02:27,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document JDmPuZkB6MMnAM0IOjcU with 693 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  52%|█████▏    | 25/48 [01:01<01:43,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document IzmPuZkB6MMnAM0IOTfF with 15 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  54%|█████▍    | 26/48 [01:02<01:10,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document IjmPuZkB6MMnAM0IOTeH with 5 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  56%|█████▋    | 27/48 [01:03<00:57,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document ITmPuZkB6MMnAM0IOTdM with 46 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  58%|█████▊    | 28/48 [01:04<00:41,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document IDmPuZkB6MMnAM0IOTcM with 15 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  60%|██████    | 29/48 [01:04<00:30,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document HzmPuZkB6MMnAM0IODfG with 9 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  62%|██████▎   | 30/48 [01:04<00:21,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document HjmPuZkB6MMnAM0IODd_ with 1 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  65%|██████▍   | 31/48 [01:07<00:28,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document HTmPuZkB6MMnAM0IODcx with 66 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  67%|██████▋   | 32/48 [01:16<01:01,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document HDmPuZkB6MMnAM0INzfh with 279 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  69%|██████▉   | 33/48 [01:16<00:41,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document GzmPuZkB6MMnAM0INzeh with 5 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  71%|███████   | 34/48 [01:22<00:50,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document GjmPuZkB6MMnAM0INzdY with 190 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  73%|███████▎  | 35/48 [01:33<01:17,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document GTmPuZkB6MMnAM0INjf- with 431 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  75%|███████▌  | 36/48 [01:35<00:55,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document GDmPuZkB6MMnAM0INjeq with 36 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  77%|███████▋  | 37/48 [01:36<00:40,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document FzmPuZkB6MMnAM0INjdX with 39 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  79%|███████▉  | 38/48 [01:38<00:31,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document FjmPuZkB6MMnAM0INjcM with 49 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  81%|████████▏ | 39/48 [01:39<00:22,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document FTmPuZkB6MMnAM0INTfM with 22 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  83%|████████▎ | 40/48 [01:40<00:16,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document FDmPuZkB6MMnAM0INTd_ with 30 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  85%|████████▌ | 41/48 [01:41<00:11,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document EzmPuZkB6MMnAM0INTdC with 18 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  88%|████████▊ | 42/48 [01:44<00:12,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document EjmPuZkB6MMnAM0INDf_ with 95 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  90%|████████▉ | 43/48 [01:46<00:09,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document ETmPuZkB6MMnAM0INDe3 with 56 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  92%|█████████▏| 44/48 [01:59<00:21,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document EDmPuZkB6MMnAM0INDdZ with 557 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  94%|█████████▍| 45/48 [02:00<00:11,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document DzmPuZkB6MMnAM0IMzfT with 8 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  96%|█████████▌| 46/48 [02:01<00:06,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document DjmPuZkB6MMnAM0IMzdc with 48 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  98%|█████████▊| 47/48 [02:03<00:02,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document DTmPuZkB6MMnAM0IMzcZ with 60 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings: 100%|██████████| 48/48 [02:04<00:00,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document DDmPuZkB6MMnAM0IMjey with 43 chunks\n",
      "Document processing completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def update_document_with_chunks(doc_id, doc_source, chunker_instance):\n",
    "    \"\"\"\n",
    "    Update a document with chunks and their entities using existing annotations.\n",
    "    \n",
    "    Args:\n",
    "        doc_id: Elasticsearch document ID\n",
    "        doc_source: Full document source containing text and annotations\n",
    "        chunker_instance: DocumentChunker instance\n",
    "    \"\"\"\n",
    "    text = doc_source.get(\"text\", \"\")\n",
    "    existing_annotations = doc_source.get(\"annotations\", [])\n",
    "    \n",
    "    # Generate chunks with embeddings\n",
    "    chunks = chunker_instance.generate_chunks_with_embeddings(text)\n",
    "    \n",
    "    if not chunks:\n",
    "        print(f\"Warning: No chunks generated for document {doc_id}\")\n",
    "        return\n",
    "    \n",
    "    # Add entity information to each chunk\n",
    "    for chunk in chunks:\n",
    "        chunk_text = chunk[1]\n",
    "        chunk_entities = chunker_instance.get_entities_from_chunk(\n",
    "            chunk_text, text, existing_annotations\n",
    "        )\n",
    "        chunk[2] = chunk_entities  # Update entities placeholder\n",
    "    \n",
    "    # Prepare data for Elasticsearch update\n",
    "    passages_body = [\n",
    "        {\n",
    "            \"vectors\": {\n",
    "                \"predicted_value\": chunk[0],\n",
    "                \"entities\": chunk[2], \n",
    "                \"text\": chunk[1]\n",
    "            }\n",
    "        } \n",
    "        for chunk in chunks\n",
    "    ]\n",
    "    \n",
    "    # Update document in Elasticsearch\n",
    "    try:\n",
    "        data = {\"doc\": {\"chunks\": passages_body}}\n",
    "        response = es.update(index=INDEX_NAME, id=doc_id, body=data)\n",
    "        print(f\"Updated document {doc_id} with {len(chunks)} chunks\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating document {doc_id}: {e}\")\n",
    "\n",
    "\n",
    "def process_documents_for_chunking(index_name=INDEX_NAME, query=None, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Process documents from Elasticsearch to add chunks and embeddings.\n",
    "    \n",
    "    Args:\n",
    "        index_name: Index to search\n",
    "        query: Optional query to filter documents\n",
    "        batch_size: Maximum documents to process\n",
    "    \"\"\"\n",
    "    if query is None:\n",
    "        query = {\"match_all\": {}}\n",
    "    \n",
    "    search_body = {\"query\": query}\n",
    "    \n",
    "    try:\n",
    "        print(\"Searching for documents to process...\")\n",
    "        response = es.search(index=index_name, body=search_body, size=batch_size)\n",
    "        documents = response[\"hits\"][\"hits\"]\n",
    "        \n",
    "        print(f\"Found {len(documents)} documents to process\")\n",
    "        \n",
    "        # Suppress Elasticsearch transport logging for cleaner output\n",
    "        logging.getLogger(\"elastic_transport\").setLevel(logging.WARNING)\n",
    "        \n",
    "        # Process documents in reverse order (optional)\n",
    "        documents.reverse()\n",
    "        \n",
    "        # Process each document\n",
    "        for doc in tqdm(documents, desc=\"Adding chunks and embeddings\"):\n",
    "            doc_id = doc[\"_id\"]\n",
    "            doc_source = doc[\"_source\"]\n",
    "            update_document_with_chunks(doc_id, doc_source, chunker)\n",
    "            \n",
    "        print(\"Document processing completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during document processing: {e}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "process_documents_for_chunking()  # Process all documents\n",
    "# process_documents_for_chunking(query={\"terms\": {\"id\": specific_ids}})  # Process specific documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-video-upscaler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
