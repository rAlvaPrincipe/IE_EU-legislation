{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elasticsearch Document Processing Pipeline\n",
    "\n",
    "This notebook provides a complete pipeline for processing legal documents and storing them in Elasticsearch with:\n",
    "- Text chunking and vector embeddings\n",
    "- Named Entity Recognition (NER) annotations\n",
    "- Full-text search capabilities\n",
    "- Duplicate detection and removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch Server Version: 8.13.3\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "ELASTICSEARCH_HOST = \"http://localhost:9201\"\n",
    "INDEX_NAME = \"eu_legislation\"\n",
    "JSON_FOLDER = \"./output\"\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 100\n",
    "VECTOR_DIMS = 768\n",
    "MODEL_NAME = \"Alibaba-NLP/gte-multilingual-base\"\n",
    "\n",
    "# Imports\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import torch\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import logging\n",
    "\n",
    "# Initialize Elasticsearch client\n",
    "es = Elasticsearch(ELASTICSEARCH_HOST, verify_certs=False, request_timeout=60)\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    response = es.info()\n",
    "    es_version = response[\"version\"][\"number\"]\n",
    "    print(f\"Connected to Elasticsearch Server Version: {es_version}\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_string_hash(input_string):\n",
    "    \"\"\"Generate SHA256 hash for a given string.\"\"\"\n",
    "    hash_object = hashlib.sha256()\n",
    "    hash_object.update(input_string.encode(\"utf-8\"))\n",
    "    return hash_object.hexdigest()\n",
    "\n",
    "\n",
    "def process_annotation(annotation, text, document_id):\n",
    "    \"\"\"Process a single annotation into the required format.\"\"\"\n",
    "    name = text[annotation[\"start\"]:annotation[\"end\"]]\n",
    "    \n",
    "    ann_object = {\n",
    "        \"mention\": name,\n",
    "        \"start\": annotation[\"start\"],\n",
    "        \"end\": annotation[\"end\"],\n",
    "        \"id\": annotation[\"id\"],\n",
    "        \"type\": annotation[\"type\"],\n",
    "    }\n",
    "    \n",
    "    # Handle linking information\n",
    "    if (\"linking\" in annotation.get(\"features\", {}) and \n",
    "        not annotation[\"features\"][\"linking\"].get(\"is_nil\", True)):\n",
    "        \n",
    "        linking = annotation[\"features\"][\"linking\"]\n",
    "        ann_object.update({\n",
    "            \"display_name\": annotation[\"features\"].get(\"title\", name),\n",
    "            \"is_linked\": True,\n",
    "            \"id_ER\": linking.get(\"top_candidate\", {}).get(\"url\", \"\")\n",
    "        })\n",
    "    else:\n",
    "        ann_object.update({\n",
    "            \"display_name\": name,\n",
    "            \"is_linked\": False,\n",
    "            \"id_ER\": f\"{document_id}_{name}\"\n",
    "        })\n",
    "    \n",
    "    return ann_object\n",
    "\n",
    "\n",
    "def clean_document_data(file_object):\n",
    "    \"\"\"Clean and prepare document data for indexing.\"\"\"\n",
    "    # Remove unnecessary fields\n",
    "    for key in [\"annotation_sets\", \"annoation_sets\", \"features\", \"_id\"]:\n",
    "        if key in file_object:\n",
    "            del file_object[key]\n",
    "    \n",
    "    # Ensure required fields exist\n",
    "    if \"metadata\" not in file_object:\n",
    "        file_object[\"metadata\"] = []\n",
    "    \n",
    "    return file_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample document:\n",
      "{\n",
      "  \"text\": \"CHAPTER IV\\nUNFAIR CONTRACTUAL TERMS RELATED TO DATA ACCESS AND USE BETWEEN ENTERPRISES\\nArticle 13\\nUnfair contractual terms unilaterally imposed on another enterprise\\n1. A contractual term concerning access to and the use of data or liability and remedies for the breach or the termination of data related obligations, which has been unilaterally imposed by an enterprise on another enterprise, shall not be binding on the latter enterprise if it is unfair.\\n2. A contractual term which reflects mandatory provisions of Union law, or provisions of Union law which would apply if the contractual terms did not regulate the matter, shall not be considered to be unfair.\\n3. A contractual term is unfair if it is of such a nature that its use grossly deviates from good commercial practice in data access and use, contrary to good faith and fair dealing.\\n4. In particular, a contractual term shall be unfair for the purposes of paragraph 3, if its object or effect is to:\\n(a) exclude or limit the liability of the party that unilaterally imposed the term for intentional acts or gross negligence;\\n(b) exclude the remedies available to the party upon whom the term has been unilaterally imposed in the case of nonperformance of contractual obligations, or the liability of the party that unilaterally imposed the term in the case of a breach of those obligations;\\n(c) give the party that unilaterally imposed the term the exclusive right to determine whether the data supplied are in conformity with the contract or to interpret any contractual term.\\n22.12.2023 http://data.europa.eu/eli/reg/2023/2854/oj 48/71\\n5. A contractual term shall be presumed to be unfair for the purposes of paragraph 3 if its object or effect is to:\\n(a) inappropriately limit remedies in the case of non-performance of contractual obligations or liability in the case of a breach of those obligations, or extend the liability of the enterprise upon whom the term has been unilaterally imposed;\\n(b) allow the party that unilaterally imposed the term to access and use the data of the other contracting party in a manner that is significantly detrimental to the legitimate interests of the other contracting party, in particular when such data contain commercially sensitive data or are protected by trade secrets or by intellectual property rights;\\n(c) prevent the party upon whom the term has been unilaterally imposed from using the data provided or generated by that party during the period of the contract, or to limit the use of such data to the extent that that party is not entitled to use, capture, access or control such data or exploit the value of such data in an adequate manner;\\n(d) prevent the party upon whom the term has been unilaterally imposed from terminating the agreement within a reasonable period;\\n(e) prevent the party upon whom the term has been unilaterally imposed from obtaining a copy of the data provided or generated by that party during the period of the contract or within a reasonable period after the termination thereof;\\n(f) enable the party that unilaterally imposed the term to terminate the contract at unreasonably short notice, taking into consideration any reasonable possibility of the other contracting party to switch to an alternative and comparable service and the financial detriment caused by such termination, except where there are serious grounds for so doing;\\n(g) enable the party that unilaterally imposed the term to substantially change the price specified in the contract or any other substantive condition related to the nature, format, quality or quantity of the data to be shared, where no valid reason and no right of the other party to terminate the contract in the case of such a change is specified in the contract.\\nPoint (g) of the first subparagraph shall not affect terms by which the party that unilaterally imposed the term reserves the right to unilaterally change the terms of a contract of an indeterminate duration, provided that the contract specified a valid reason for such unilateral changes, that the party that unilaterally imposed the term is required to provide the other contracting party with reasonable notice of any such intended change, and that the other contracting party is free to terminate the contract at no cost in the case of a change.\\n6. A contractual term shall be considered to be unilaterally imposed within the meaning of this Article if it has been supplied by one contracting party and the other contracting party has not been able to influence its content despite an attempt to negotiate it.\\nThe contracting party that supplied the contractual term bears the burden of proving that that term has not been unilaterally imposed.\\nThe contracting party that supplied the contested contractual term may not argue that the term is an unfair contractual term.\\n7. Where the unfair contractual term is severable from the remaining terms of the contract, those remaining terms shall be binding.\\n8. This Article does not apply to contractual terms defining the main subject matter of the contract or to the adequacy of the price, as against the data supplied in exchange.\\n9. The parties to a contract covered by paragraph 1 shall not exclude the application of this Article, derogate from it, or vary its effects.\\n\",\n",
      "  \"offset_type\": \"p\",\n",
      "  \"name\": \"DataAct_Chapter_IV\",\n",
      "  \"id\": \"0b725ac132b5644170eb43a0d577b6e5f99b1996dc8516bcabd3f52b759266c7\",\n",
      "  \"metadata\": []\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d4/l_fskgld3vddb2mq0h787wd40000gn/T/ipykernel_95037/352670684.py:13: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  return es.search(index=INDEX_NAME, body=search_query)\n"
     ]
    }
   ],
   "source": [
    "# Quick document search and inspection\n",
    "def search_documents(query=\"*\", size=10, exclude_fields=None):\n",
    "    \"\"\"Search documents in the index with optional field exclusions.\"\"\"\n",
    "    if exclude_fields is None:\n",
    "        exclude_fields = [\"chunks\", \"annotations\"]\n",
    "    \n",
    "    search_query = {\n",
    "        \"query\": {\"query_string\": {\"query\": query}},\n",
    "        \"_source\": {\"excludes\": exclude_fields},\n",
    "        \"size\": size,\n",
    "    }\n",
    "    \n",
    "    return es.search(index=INDEX_NAME, body=search_query)\n",
    "\n",
    "\n",
    "def find_empty_annotation_documents():\n",
    "    \"\"\"Find documents with empty annotations field.\"\"\"\n",
    "    search_query = {\n",
    "        \"query\": {\"query_string\": {\"query\": \"*\"}},\n",
    "        \"_source\": {\"excludes\": [\"chunks\", \"annotation_sets\"]},\n",
    "    }\n",
    "    \n",
    "    response = es.search(index=INDEX_NAME, body=search_query)\n",
    "    empty_annotation_ids = []\n",
    "    \n",
    "    for hit in response[\"hits\"][\"hits\"]:\n",
    "        if hit[\"_source\"].get(\"annotations\") == []:\n",
    "            name = hit[\"_source\"].get(\"name\", \"(no name)\")\n",
    "            print(f\"Document with empty 'annotations' field: {name}\")\n",
    "            empty_annotation_ids.append(hit[\"_source\"][\"id\"])\n",
    "    \n",
    "    return empty_annotation_ids\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sample_response = search_documents(size=1)\n",
    "if sample_response[\"hits\"][\"hits\"]:\n",
    "    print(\"Sample document:\")\n",
    "    print(json.dumps(sample_response[\"hits\"][\"hits\"][0][\"_source\"], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing index: eu_legislation\n",
      "Created index: eu_legislation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d4/l_fskgld3vddb2mq0h787wd40000gn/T/ipykernel_95037/1619132199.py:65: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  response = es.indices.create(index=index_name, body=index_settings)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'eu_legislation'})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_index_settings():\n",
    "    \"\"\"Get the index settings with custom nested object limit.\"\"\"\n",
    "    return {\n",
    "        \"settings\": {\n",
    "            \"index.mapping.nested_objects.limit\": 20000\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"text\": {\"type\": \"text\"},\n",
    "                \"name\": {\"type\": \"keyword\"},\n",
    "                \"preview\": {\"type\": \"keyword\"},\n",
    "                \"id\": {\"type\": \"keyword\"},\n",
    "                \"metadata\": {\n",
    "                    \"type\": \"nested\",\n",
    "                    \"properties\": {\n",
    "                        \"type\": {\"type\": \"keyword\"},\n",
    "                        \"value\": {\"type\": \"keyword\"}\n",
    "                    }\n",
    "                },\n",
    "                \"annotations\": {\n",
    "                    \"type\": \"nested\",\n",
    "                    \"properties\": {\n",
    "                        \"mention\": {\"type\": \"keyword\"},\n",
    "                        \"start\": {\"type\": \"integer\"},\n",
    "                        \"end\": {\"type\": \"integer\"},\n",
    "                        \"display_name\": {\"type\": \"keyword\"},\n",
    "                        \"id\": {\"type\": \"integer\"},\n",
    "                        \"type\": {\"type\": \"keyword\"},\n",
    "                        \"is_linked\": {\"type\": \"boolean\"},\n",
    "                        \"id_ER\": {\"type\": \"keyword\"}\n",
    "                    }\n",
    "                },\n",
    "                \"chunks\": {\n",
    "                    \"type\": \"nested\",\n",
    "                    \"properties\": {\n",
    "                        \"vectors\": {\n",
    "                            \"type\": \"nested\",\n",
    "                            \"properties\": {\n",
    "                                \"predicted_value\": {\n",
    "                                    \"type\": \"dense_vector\",\n",
    "                                    \"index\": True,\n",
    "                                    \"dims\": VECTOR_DIMS,\n",
    "                                    \"similarity\": \"cosine\",\n",
    "                                },\n",
    "                                \"text\": {\"type\": \"text\"},\n",
    "                                \"entities\": {\"type\": \"text\"},\n",
    "                            },\n",
    "                        },\n",
    "                    },\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def recreate_index(index_name=INDEX_NAME, delete_existing=False):\n",
    "    \"\"\"Create or recreate the Elasticsearch index.\"\"\"\n",
    "    try:\n",
    "        if delete_existing and es.indices.exists(index=index_name):\n",
    "            es.indices.delete(index=index_name)\n",
    "            print(f\"Deleted existing index: {index_name}\")\n",
    "        \n",
    "        if not es.indices.exists(index=index_name):\n",
    "            index_settings = get_index_settings()\n",
    "            response = es.indices.create(index=index_name, body=index_settings)\n",
    "            print(f\"Created index: {index_name}\")\n",
    "            return response\n",
    "        else:\n",
    "            print(f\"Index {index_name} already exists\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error managing index: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Uncomment the line below to recreate the index\n",
    "recreate_index(delete_existing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_files(path, target_ids=None):\n",
    "    \"\"\"\n",
    "    Read and process JSON annotation files from a directory.\n",
    "    \n",
    "    Args:\n",
    "        path: Directory containing .json.annotated files\n",
    "        target_ids: Optional set of document IDs to filter by\n",
    "        \n",
    "    Returns:\n",
    "        List of processed document objects\n",
    "    \"\"\"\n",
    "    json_files = [f for f in os.listdir(path) if f.endswith(\".json\")]\n",
    "    data = []\n",
    "    \n",
    "    print(f\"Processing {len(json_files)} JSON files from {path}\")\n",
    "    \n",
    "    for json_file in tqdm(json_files, desc=\"Reading files\"):\n",
    "        try:\n",
    "            with open(os.path.join(path, json_file), \"r\") as file:\n",
    "                file_object = json.load(file)\n",
    "                \n",
    "                # Skip empty files\n",
    "                if not file_object.get(\"text\"):\n",
    "                    print(f\"Warning: Skipping empty file: {json_file}\")\n",
    "                    continue\n",
    "                \n",
    "                # Generate document ID\n",
    "                file_object[\"id\"] = get_string_hash(file_object[\"text\"])\n",
    "                \n",
    "                # Filter by target IDs if provided\n",
    "                if target_ids and file_object[\"id\"] not in target_ids:\n",
    "                    continue\n",
    "                \n",
    "                # Process annotations\n",
    "                annotations = process_document_annotations(file_object)\n",
    "                file_object[\"annotations\"] = annotations\n",
    "                \n",
    "                # Clean up the document\n",
    "                file_object = clean_document_data(file_object)\n",
    "                \n",
    "                data.append(file_object)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {json_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Successfully processed {len(data)} documents\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def process_document_annotations(file_object):\n",
    "    \"\"\"Extract and process annotations from a document.\"\"\"\n",
    "    text = file_object.get(\"text\", \"\")\n",
    "    annotations = []\n",
    "    \n",
    "    annotation_sets = file_object.get(\"annotation_sets\", {})\n",
    "    entities = annotation_sets.get(\"entities_\", {})\n",
    "    raw_annotations = entities.get(\"annotations\", [])\n",
    "    \n",
    "    for annotation in raw_annotations:\n",
    "        try:\n",
    "            ann_object = process_annotation(annotation, text, file_object.get(\"id\", \"\"))\n",
    "            annotations.append(ann_object)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error processing annotation: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Processed {len(annotations)} annotations for document '{file_object.get('name', 'Unknown')}'\")\n",
    "    return annotations\n",
    "\n",
    "\n",
    "def send_to_elasticsearch(data, index_name=INDEX_NAME, update_existing=True):\n",
    "    \"\"\"\n",
    "    Send documents to Elasticsearch with optional duplicate handling.\n",
    "    \n",
    "    Args:\n",
    "        data: List of document objects\n",
    "        index_name: Target index name\n",
    "        update_existing: Whether to update existing documents\n",
    "    \"\"\"\n",
    "    print(f\"Sending {len(data)} documents to Elasticsearch...\")\n",
    "    \n",
    "    for item in tqdm(data, desc=\"Indexing documents\"):\n",
    "        try:\n",
    "            if update_existing:\n",
    "                # Remove existing documents with same ID\n",
    "                search_query = {\"query\": {\"term\": {\"id\": item[\"id\"]}}}\n",
    "                search_response = es.search(index=index_name, body=search_query)\n",
    "                \n",
    "                for hit in search_response[\"hits\"][\"hits\"]:\n",
    "                    es.delete(index=index_name, id=hit[\"_id\"])\n",
    "            \n",
    "            # Index the new document\n",
    "            es.index(index=index_name, body=item)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error indexing document {item.get('name', 'Unknown')}: {e}\")\n",
    "    \n",
    "    print(\"Document indexing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 48 JSON files from ./output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading files: 100%|██████████| 48/48 [00:00<00:00, 438.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 122 annotations for document 'DataAct_Chapter_IX'\n",
      "Processed 158 annotations for document 'DataGovernanceAct_Chapter_II'\n",
      "Processed 134 annotations for document 'DataAct_Chapter_VIII'\n",
      "Processed 49 annotations for document 'AIAct_Chapter_XI'\n",
      "Processed 631 annotations for document 'AIAct_Chapter_III'\n",
      "Processed 183 annotations for document 'DataGovernanceAct_Chapter_III'\n",
      "Processed 127 annotations for document 'AIAct_Chapter_I'\n",
      "Processed 67 annotations for document 'GDPR_Chapter_IX'\n",
      "Processed 88 annotations for document 'DataGovernanceAct_Chapter_I'\n",
      "Processed 59 annotations for document 'DataGovernanceAct_Chapter_VI'\n",
      "Processed 137 annotations for document 'DataGovernanceAct_Chapter_IV'\n",
      "Processed 104 annotations for document 'DataAct_Chapter_XI'\n",
      "Processed 204 annotations for document 'DataAct_Chapter_III'\n",
      "Processed 664 annotations for document 'DataAct-intro'\n",
      "Processed 501 annotations for document 'AIAct_Chapter_IX'\n",
      "Processed 24 annotations for document 'GDPR_Chapter_X'\n",
      "Processed 221 annotations for document 'AIAct_Chapter_XIII'\n",
      "Processed 312 annotations for document 'GDPR_Chapter_III'\n",
      "Processed 5 annotations for document 'DataAct_Chapter_X'\n",
      "Processed 59 annotations for document 'GDPR_Chapter_XI'\n",
      "Processed 28 annotations for document 'DataGovernanceAct_Chapter_V'\n",
      "Processed 153 annotations for document 'AIAct_Chapter_V'\n",
      "Processed 30 annotations for document 'DataGovernanceAct_Chapter_VIII'\n",
      "Processed 25 annotations for document 'AIAct_Chapter_IV'\n",
      "Processed 398 annotations for document 'AIAct_Chapter_intro'\n",
      "Processed 42 annotations for document 'DataAct_Chapter_VII'\n",
      "Processed 194 annotations for document 'GDPR_Chapter_VI'\n",
      "Processed 34 annotations for document 'DataGovernanceAct_Chapter_IX'\n",
      "Processed 589 annotations for document 'DataAct_Chapter_II'\n",
      "Processed 113 annotations for document 'GDPR_Chapter_VIII'\n",
      "Processed 80 annotations for document 'GDPR_Chapter_II'\n",
      "Processed 29 annotations for document 'AIAct_Chapter_X'\n",
      "Processed 293 annotations for document 'GDPR_Chapter_VII'\n",
      "Processed 140 annotations for document 'DataAct_Chapter_VI'\n",
      "Processed 553 annotations for document 'DataAct_Chapter_V'\n",
      "Processed 150 annotations for document 'GDPR_Chapter_V'\n",
      "Processed 74 annotations for document 'AIAct_Chapter_II'\n",
      "Processed 34 annotations for document 'DataAct_Chapter_IV'\n",
      "Processed 125 annotations for document 'AIAct_Chapter_VII'\n",
      "Processed 7 annotations for document 'GDPR_intro'\n",
      "Processed 162 annotations for document 'AIAct_Chapter_XII'\n",
      "Processed 40 annotations for document 'DataGovernanceAct_Chapter_VII'\n",
      "Processed 29 annotations for document 'AIAct_Chapter_VIII'\n",
      "Processed 466 annotations for document 'GDPR_Chapter_IV'\n",
      "Processed 156 annotations for document 'GDPR_Chapter_I'\n",
      "Processed 537 annotations for document 'AIAct_Chapter_VI'\n",
      "Processed 501 annotations for document 'DataGovernanceAct_Chapter_intro'\n",
      "Processed 115 annotations for document 'DataAct_Chapter_I'\n",
      "Successfully processed 48 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process documents - you can filter by specific document IDs if needed\n",
    "# empty_annotation_ids = find_empty_annotation_documents()  # Uncomment to filter\n",
    "target_ids = None  # or set to empty_annotation_ids to process only those documents\n",
    "\n",
    "data = read_json_files(JSON_FOLDER, target_ids=target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded {len(data)} documents ready for processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending 48 documents to Elasticsearch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing documents:   0%|          | 0/48 [00:00<?, ?it/s]/var/folders/d4/l_fskgld3vddb2mq0h787wd40000gn/T/ipykernel_95037/2021533814.py:88: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  search_response = es.search(index=index_name, body=search_query)\n",
      "/var/folders/d4/l_fskgld3vddb2mq0h787wd40000gn/T/ipykernel_95037/2021533814.py:94: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use the 'document' parameter. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  es.index(index=index_name, body=item)\n",
      "Indexing documents: 100%|██████████| 48/48 [00:04<00:00,  9.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document indexing completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "send_to_elasticsearch(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Chunking and Vector Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping for chunk vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This mapping update is now included in the main index creation\n",
    "# No need to run separately if using recreate_index() function\n",
    "print(\"Vector mapping is included in the main index configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: Alibaba-NLP/gte-multilingual-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "class DocumentChunker:\n",
    "    \"\"\"Handles document chunking and embedding generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=MODEL_NAME, device=\"mps\"):\n",
    "        \"\"\"Initialize the chunker with sentence transformer model.\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self._model = None\n",
    "        self._initialize_model()\n",
    "    \n",
    "    def _initialize_model(self):\n",
    "        \"\"\"Lazy initialization of the sentence transformer model.\"\"\"\n",
    "        if self._model is None:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self._model = SentenceTransformer(\n",
    "                self.model_name, \n",
    "                trust_remote_code=True\n",
    "            ).to(self.device)\n",
    "            print(\"Model loaded successfully\")\n",
    "    \n",
    "    def generate_chunks_with_embeddings(self, text):\n",
    "        \"\"\"\n",
    "        Split text into chunks and generate embeddings for each chunk.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to chunk and embed\n",
    "            \n",
    "        Returns:\n",
    "            List of lists: [embedding, chunk_text, entities_placeholder]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Split text into chunks\n",
    "            chunks = text_splitter.split_text(text)\n",
    "            \n",
    "            if not chunks:\n",
    "                print(\"Warning: No chunks generated from text\")\n",
    "                return []\n",
    "            \n",
    "            # Generate embeddings\n",
    "            embeddings = self._model.encode(chunks, show_progress_bar=False)\n",
    "            \n",
    "            # Return as list of lists (mutable) instead of tuples (immutable)\n",
    "            result = [[emb.tolist(), chunk, \"\"] for emb, chunk in zip(embeddings, chunks)]\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in chunking/embedding: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_entities_from_chunk(self, chunk_text, full_text, annotations):\n",
    "        \"\"\"\n",
    "        Find entities from annotations that are present in the chunk text.\n",
    "        \n",
    "        Args:\n",
    "            chunk_text: The text of the current chunk\n",
    "            full_text: The full document text \n",
    "            annotations: List of annotation objects\n",
    "        \n",
    "        Returns:\n",
    "            String of entity mentions found in the chunk\n",
    "        \"\"\"\n",
    "        chunk_entities = []\n",
    "        \n",
    "        # Find the position of this chunk in the full text\n",
    "        chunk_start_in_full = full_text.find(chunk_text)\n",
    "        if chunk_start_in_full == -1:\n",
    "            return \"\"\n",
    "        \n",
    "        chunk_end_in_full = chunk_start_in_full + len(chunk_text)\n",
    "        \n",
    "        # Check which annotations overlap with this chunk\n",
    "        for annotation in annotations:\n",
    "            ann_start = annotation.get(\"start\", 0)\n",
    "            ann_end = annotation.get(\"end\", 0)\n",
    "            \n",
    "            # Check if annotation overlaps with chunk boundaries\n",
    "            if ((ann_start >= chunk_start_in_full and ann_start < chunk_end_in_full) or \n",
    "                (ann_end > chunk_start_in_full and ann_end <= chunk_end_in_full) or \n",
    "                (ann_start <= chunk_start_in_full and ann_end >= chunk_end_in_full)):\n",
    "                \n",
    "                entity_mention = full_text[ann_start:ann_end]\n",
    "                chunk_entities.append(entity_mention)\n",
    "        \n",
    "        return \" \".join(chunk_entities)\n",
    "\n",
    "\n",
    "# Initialize the chunker\n",
    "chunker = DocumentChunker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for documents to process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d4/l_fskgld3vddb2mq0h787wd40000gn/T/ipykernel_95037/3966881953.py:65: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  response = es.search(index=index_name, body=search_body, size=batch_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48 documents to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:   0%|          | 0/48 [00:00<?, ?it/s]/var/folders/d4/l_fskgld3vddb2mq0h787wd40000gn/T/ipykernel_95037/3966881953.py:43: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  response = es.update(index=INDEX_NAME, id=doc_id, body=data)\n",
      "Adding chunks and embeddings:   2%|▏         | 1/48 [01:44<1:21:57, 104.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document CzlguZkB6MMnAM0I3DcJ with 78 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:   4%|▍         | 2/48 [02:14<46:34, 60.75s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document CjlguZkB6MMnAM0I2ze6 with 255 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:   6%|▋         | 3/48 [02:21<27:12, 36.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document CTlguZkB6MMnAM0I2zdQ with 168 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:   8%|▊         | 4/48 [02:24<16:56, 23.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document CDlguZkB6MMnAM0I2jfu with 29 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  10%|█         | 5/48 [02:34<13:03, 18.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document BzlguZkB6MMnAM0I2jec with 120 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  12%|█▎        | 6/48 [02:35<08:37, 12.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document BjlguZkB6MMnAM0I2jc8 with 6 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  15%|█▍        | 7/48 [02:35<05:46,  8.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document BTlguZkB6MMnAM0I2Tfi with 12 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  17%|█▋        | 8/48 [02:37<04:16,  6.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document BDlguZkB6MMnAM0I2TeU with 48 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  19%|█▉        | 9/48 [02:38<02:57,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document AzlguZkB6MMnAM0I2TdH with 3 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  21%|██        | 10/48 [02:40<02:29,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document AjlguZkB6MMnAM0I2Df7 with 68 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  23%|██▎       | 11/48 [02:41<01:46,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document ATlguZkB6MMnAM0I2Dd3 with 15 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  25%|██▌       | 12/48 [02:42<01:24,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document ADlguZkB6MMnAM0I2Dcq with 34 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  27%|██▋       | 13/48 [02:45<01:30,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document _zlguZkB6MMnAM0I1zbd with 49 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  29%|██▉       | 14/48 [02:49<01:40,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document _jlguZkB6MMnAM0I1zZ0 with 105 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  31%|███▏      | 15/48 [02:50<01:22,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document _TlguZkB6MMnAM0I1jb1 with 44 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  33%|███▎      | 16/48 [02:53<01:25,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document _DlguZkB6MMnAM0I1jao with 83 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  35%|███▌      | 17/48 [02:54<01:05,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document -zlguZkB6MMnAM0I1jZa with 16 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  38%|███▊      | 18/48 [02:56<01:03,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document -jlguZkB6MMnAM0I1Tb- with 37 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  40%|███▉      | 19/48 [02:58<00:59,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document -TlguZkB6MMnAM0I1Ta2 with 33 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  42%|████▏     | 20/48 [03:02<01:08,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document -DlguZkB6MMnAM0I1TYF with 113 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  44%|████▍     | 21/48 [03:02<00:52,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 9zlguZkB6MMnAM0I1Das with 20 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  46%|████▌     | 22/48 [03:05<00:54,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 9jlguZkB6MMnAM0I1DZq with 44 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  48%|████▊     | 23/48 [03:05<00:41,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 9TlguZkB6MMnAM0I1DYl with 13 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  50%|█████     | 24/48 [03:24<02:42,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 9DlguZkB6MMnAM0I0zbJ with 693 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  52%|█████▏    | 25/48 [03:25<01:56,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 8zlguZkB6MMnAM0I0zZP with 15 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  54%|█████▍    | 26/48 [03:25<01:19,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 8jlguZkB6MMnAM0I0zYM with 5 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  56%|█████▋    | 27/48 [03:27<01:03,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 8TlguZkB6MMnAM0I0jbK with 46 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  58%|█████▊    | 28/48 [03:28<00:46,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 8DlguZkB6MMnAM0I0jZ8 with 15 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  62%|██████▎   | 30/48 [03:28<00:23,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 7zlguZkB6MMnAM0I0jYx with 9 chunks\n",
      "Updated document 7jlguZkB6MMnAM0I0Tbj with 1 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  65%|██████▍   | 31/48 [03:31<00:29,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 7TlguZkB6MMnAM0I0TaD with 66 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  67%|██████▋   | 32/48 [03:40<01:00,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 7DlguZkB6MMnAM0I0TYS with 279 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  69%|██████▉   | 33/48 [03:40<00:43,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 6zlguZkB6MMnAM0I0Da1 with 5 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  71%|███████   | 34/48 [03:46<00:49,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 6jlguZkB6MMnAM0I0DZq with 190 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  73%|███████▎  | 35/48 [03:57<01:15,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 6TlguZkB6MMnAM0Izzbo with 431 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  75%|███████▌  | 36/48 [03:59<00:58,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 6DlguZkB6MMnAM0IzzZL with 36 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  77%|███████▋  | 37/48 [04:01<00:42,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 5zlguZkB6MMnAM0Izjbt with 39 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  79%|███████▉  | 38/48 [04:03<00:31,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 5jlguZkB6MMnAM0Izjad with 49 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  81%|████████▏ | 39/48 [04:03<00:22,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 5TlguZkB6MMnAM0IzjZE with 22 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  83%|████████▎ | 40/48 [04:04<00:16,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 5DlguZkB6MMnAM0IzTb1 with 30 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  85%|████████▌ | 41/48 [04:05<00:11,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 4zlguZkB6MMnAM0IzTao with 18 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  88%|████████▊ | 42/48 [04:08<00:12,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 4jlguZkB6MMnAM0IzTZM with 95 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  90%|████████▉ | 43/48 [04:10<00:09,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 4TlguZkB6MMnAM0IzDb3 with 56 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  92%|█████████▏| 44/48 [04:23<00:21,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 4DlguZkB6MMnAM0IzDaC with 557 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  94%|█████████▍| 45/48 [04:24<00:12,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 3zlguZkB6MMnAM0Iyzai with 8 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  96%|█████████▌| 46/48 [04:25<00:06,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 3jlguZkB6MMnAM0Iyjb_ with 48 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings:  98%|█████████▊| 47/48 [04:27<00:02,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 3TlguZkB6MMnAM0IyjZ8 with 60 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding chunks and embeddings: 100%|██████████| 48/48 [04:29<00:00,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document 3DlguZkB6MMnAM0IyTb5 with 43 chunks\n",
      "Document processing completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def update_document_with_chunks(doc_id, doc_source, chunker_instance):\n",
    "    \"\"\"\n",
    "    Update a document with chunks and their entities using existing annotations.\n",
    "    \n",
    "    Args:\n",
    "        doc_id: Elasticsearch document ID\n",
    "        doc_source: Full document source containing text and annotations\n",
    "        chunker_instance: DocumentChunker instance\n",
    "    \"\"\"\n",
    "    text = doc_source.get(\"text\", \"\")\n",
    "    existing_annotations = doc_source.get(\"annotations\", [])\n",
    "    \n",
    "    # Generate chunks with embeddings\n",
    "    chunks = chunker_instance.generate_chunks_with_embeddings(text)\n",
    "    \n",
    "    if not chunks:\n",
    "        print(f\"Warning: No chunks generated for document {doc_id}\")\n",
    "        return\n",
    "    \n",
    "    # Add entity information to each chunk\n",
    "    for chunk in chunks:\n",
    "        chunk_text = chunk[1]\n",
    "        chunk_entities = chunker_instance.get_entities_from_chunk(\n",
    "            chunk_text, text, existing_annotations\n",
    "        )\n",
    "        chunk[2] = chunk_entities  # Update entities placeholder\n",
    "    \n",
    "    # Prepare data for Elasticsearch update\n",
    "    passages_body = [\n",
    "        {\n",
    "            \"vectors\": {\n",
    "                \"predicted_value\": chunk[0],\n",
    "                \"entities\": chunk[2], \n",
    "                \"text\": chunk[1]\n",
    "            }\n",
    "        } \n",
    "        for chunk in chunks\n",
    "    ]\n",
    "    \n",
    "    # Update document in Elasticsearch\n",
    "    try:\n",
    "        data = {\"doc\": {\"chunks\": passages_body}}\n",
    "        response = es.update(index=INDEX_NAME, id=doc_id, body=data)\n",
    "        print(f\"Updated document {doc_id} with {len(chunks)} chunks\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating document {doc_id}: {e}\")\n",
    "\n",
    "\n",
    "def process_documents_for_chunking(index_name=INDEX_NAME, query=None, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Process documents from Elasticsearch to add chunks and embeddings.\n",
    "    \n",
    "    Args:\n",
    "        index_name: Index to search\n",
    "        query: Optional query to filter documents\n",
    "        batch_size: Maximum documents to process\n",
    "    \"\"\"\n",
    "    if query is None:\n",
    "        query = {\"match_all\": {}}\n",
    "    \n",
    "    search_body = {\"query\": query}\n",
    "    \n",
    "    try:\n",
    "        print(\"Searching for documents to process...\")\n",
    "        response = es.search(index=index_name, body=search_body, size=batch_size)\n",
    "        documents = response[\"hits\"][\"hits\"]\n",
    "        \n",
    "        print(f\"Found {len(documents)} documents to process\")\n",
    "        \n",
    "        # Suppress Elasticsearch transport logging for cleaner output\n",
    "        logging.getLogger(\"elastic_transport\").setLevel(logging.WARNING)\n",
    "        \n",
    "        # Process documents in reverse order (optional)\n",
    "        documents.reverse()\n",
    "        \n",
    "        # Process each document\n",
    "        for doc in tqdm(documents, desc=\"Adding chunks and embeddings\"):\n",
    "            doc_id = doc[\"_id\"]\n",
    "            doc_source = doc[\"_source\"]\n",
    "            update_document_with_chunks(doc_id, doc_source, chunker)\n",
    "            \n",
    "        print(\"Document processing completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during document processing: {e}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "process_documents_for_chunking()  # Process all documents\n",
    "# process_documents_for_chunking(query={\"terms\": {\"id\": specific_ids}})  # Process specific documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maintenance and Utility Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_by_id(client, index_name):\n",
    "    \"\"\"\n",
    "    Remove duplicate documents based on the 'id' field, keeping only one copy.\n",
    "    \n",
    "    Args:\n",
    "        client: Elasticsearch client\n",
    "        index_name: Target index name\n",
    "    \"\"\"\n",
    "    print(\"Scanning for duplicate documents...\")\n",
    "    \n",
    "    # Step 1: Gather all document IDs grouped by 'id' field\n",
    "    id_groups = {}\n",
    "    for hit in scan(client, index=index_name, query={\"_source\": [\"id\"], \"query\": {\"match_all\": {}}}):\n",
    "        doc_id = hit[\"_source\"].get(\"id\")\n",
    "        es_id = hit[\"_id\"]\n",
    "        if not doc_id:\n",
    "            continue\n",
    "        id_groups.setdefault(doc_id, []).append(es_id)\n",
    "\n",
    "    # Step 2: Prepare deletions - keep only one doc per unique 'id'\n",
    "    actions = []\n",
    "    duplicate_count = 0\n",
    "    \n",
    "    for doc_id, es_ids in id_groups.items():\n",
    "        if len(es_ids) > 1:\n",
    "            duplicate_count += len(es_ids) - 1\n",
    "            # Keep the first, delete the rest\n",
    "            duplicates = es_ids[1:]\n",
    "            for dup_id in duplicates:\n",
    "                actions.append({\n",
    "                    \"_op_type\": \"delete\", \n",
    "                    \"_index\": index_name, \n",
    "                    \"_id\": dup_id\n",
    "                })\n",
    "\n",
    "    # Step 3: Bulk delete duplicates\n",
    "    if actions:\n",
    "        print(f\"Deleting {duplicate_count} duplicate documents...\")\n",
    "        success, _ = bulk(client, actions, refresh=True)\n",
    "        print(f\"Deleted {success} duplicate documents\")\n",
    "    else:\n",
    "        print(\"No duplicates found\")\n",
    "\n",
    "\n",
    "def copy_index_data(source_index, dest_index, client=es):\n",
    "    \"\"\"\n",
    "    Copy all documents from source index to destination index.\n",
    "    \n",
    "    Args:\n",
    "        source_index: Source index name\n",
    "        dest_index: Destination index name\n",
    "        client: Elasticsearch client\n",
    "    \"\"\"\n",
    "    print(f\"Copying data from '{source_index}' to '{dest_index}'...\")\n",
    "    \n",
    "    # Copy all documents\n",
    "    actions = []\n",
    "    doc_count = 0\n",
    "    \n",
    "    for doc in scan(client, index=source_index, query={\"query\": {\"match_all\": {}}}):\n",
    "        doc_body = doc[\"_source\"]\n",
    "        actions.append({\n",
    "            \"_op_type\": \"index\",\n",
    "            \"_index\": dest_index,\n",
    "            \"_id\": doc.get(\"_id\"),\n",
    "            **doc_body\n",
    "        })\n",
    "        doc_count += 1\n",
    "\n",
    "    print(f\"Prepared {doc_count} documents for transfer\")\n",
    "\n",
    "    if actions:\n",
    "        bulk(client, actions, request_timeout=300)\n",
    "        print(\"Bulk transfer completed successfully\")\n",
    "    else:\n",
    "        print(\"Warning: No documents found to transfer\")\n",
    "\n",
    "\n",
    "def recreate_annotations_from_raw(index_name, client=es):\n",
    "    \"\"\"\n",
    "    Recreate annotations from raw annotation_sets data for all documents.\n",
    "    Useful when annotation format needs to be updated.\n",
    "    \n",
    "    Args:\n",
    "        index_name: Target index name\n",
    "        client: Elasticsearch client\n",
    "    \"\"\"\n",
    "    print(f\"Recreating annotations for index '{index_name}'...\")\n",
    "    \n",
    "    count = 0\n",
    "    for doc in scan(client, index=index_name, query={\"query\": {\"match_all\": {}}}):\n",
    "        doc_id = doc[\"_id\"]\n",
    "        source = doc[\"_source\"]\n",
    "        \n",
    "        # Skip if no annotation_sets\n",
    "        if \"annotation_sets\" not in source:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            new_annotations = process_document_annotations(source)\n",
    "            client.update(\n",
    "                index=index_name, \n",
    "                id=doc_id, \n",
    "                body={\"doc\": {\"annotations\": new_annotations}}\n",
    "            )\n",
    "            count += 1\n",
    "            \n",
    "            if count % 100 == 0:\n",
    "                print(f\"Processed {count} documents...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error updating doc {doc_id}: {e}\")\n",
    "\n",
    "    print(f\"Total documents updated: {count}\")\n",
    "\n",
    "\n",
    "# Example usage (uncomment to use):\n",
    "# remove_duplicates_by_id(es, INDEX_NAME)\n",
    "# copy_index_data(\"source_index\", \"dest_index\")\n",
    "# recreate_annotations_from_raw(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Workflow Example\n",
    "\n",
    "Follow these steps to process your documents from start to finish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Processing Workflow\n",
    "# Follow these steps in order:\n",
    "\n",
    "print(\"ELASTICSEARCH DOCUMENT PROCESSING WORKFLOW\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Step 1: Create/recreate the index\n",
    "print(\"\\n1. Setting up Elasticsearch index...\")\n",
    "# recreate_index(delete_existing=True)  # Uncomment to recreate index\n",
    "\n",
    "# Step 2: Process and load documents\n",
    "print(\"\\n2. Processing JSON files...\")\n",
    "# data = read_json_files(JSON_FOLDER)\n",
    "# print(f\"Loaded {len(data)} documents\")\n",
    "\n",
    "# Step 3: Send documents to Elasticsearch\n",
    "print(\"\\n3. Indexing documents...\")\n",
    "# send_to_elasticsearch(data)\n",
    "\n",
    "# Step 4: Add chunks and embeddings\n",
    "print(\"\\n4. Adding chunks and embeddings...\")\n",
    "# process_documents_for_chunking()\n",
    "\n",
    "# Step 5: Clean up duplicates\n",
    "print(\"\\n5. Removing duplicates...\")\n",
    "# remove_duplicates_by_id(es, INDEX_NAME)\n",
    "\n",
    "print(\"\\nWorkflow template ready!\")\n",
    "print(\"Note: Uncomment the lines above to execute each step\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-video-upscaler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
